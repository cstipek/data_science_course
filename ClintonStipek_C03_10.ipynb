{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operationalization of machine learning models\n",
    "\n",
    "In this notebook, we cover some of the important themes around model operationalization. This is an extensive topic, and we do not try to be comprehensive here. Instead we learn about some essentials and look at an example of a library that makes this kind of work very easy for us: the `mlflow` library. To introduce you to the library, we go over their [own example](https://www.mlflow.org/docs/latest/tutorials-and-examples/tutorial.html) for running an experiment. But first some vocabulary:\n",
    "\n",
    "- A **script** is some Python code we want to run, stored as a `.py` or `.ipynb` formats. Usually, the script has a set of required or optional inputs we provide (just like a Python function). In `mlflow`, we refer to these inputs as **parameters**, but do NOT confuse this term with model parameters in ML.\n",
    "- A **run** is what we call when we fix the inputs of a script to some value and executing the script. In the context of ML, the script could be a training script, its \"parameters\" could be hyper-parameters to the model we wish to train, and a run is when we train a model with the hyper-parameters set to some fix values.\n",
    "- As part of a run we can log the **parameters** we used, the **metrics** we calculated such as training and test accuracy, and **artifacts** such as plots, tables, or trained models we save externally for reuse later. We can refer to these as run meta-data. In addition to the meta-data we log explicitly in the code, `mlflow` also logs some of its own meta-data such as run ID or run time.\n",
    "- A **experiment** is a collection of related runs. So to continue with the above example, if we execute the script several times each time using another set of values for the hyper-parameters, then the experiment is the collection of all such runs. After executing all the runs, we can go to our experiment to compare them in terms of accuracy, run time, or whatever **metric** of interest.\n",
    "\n",
    "Note that the example we provide above is a \"typical\" example, and this is what we show in this notebook. But in general we can be flexible in what exactly we define as an experiment. The general idea is that from run to run, we change things and later we want to see what worked and what didn't by looking at metrics or artifacts generated by the model. A machine learning project can consist of one or several experiments. It all depends on the complexity of the proect, and how granular we think of individual runs. This is to some extent a matter of preference and can even be driven by business needs. \n",
    "\n",
    "Finally of course we can do a lot of this manually. After all we know how to run scripts with different inputs, or how to save plots or models on disk. Using a **version control** tool like Git, we can also track changes to the code. So why do we need `mlflow`? The answer is simple: It takes away most of the hassle that comes with doing such things manually, and on top of that it provides us with a UI where we go to find all our runs and quickly compare them. There are other concepts in `mlflow` that we do not cover here, but we invite you to check out [their website](https://mlflow.org/).\n",
    "\n",
    "To begin with, we create a folder to save not only the code, but also the meta-data generated by our runs. Once we begin to log runs, the project folder will be populated by such meta-data. You are advised against deleting the meta-data directly (the better way is to use the UI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Using cached mlflow-1.26.1-py3-none-any.whl (17.8 MB)\n",
      "Collecting Flask\n",
      "  Using cached Flask-2.1.2-py3-none-any.whl (95 kB)\n",
      "Collecting querystring-parser\n",
      "  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting gitpython>=2.1.0\n",
      "  Using cached GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from mlflow) (21.2)\n",
      "Collecting databricks-cli>=0.8.7\n",
      "  Downloading databricks-cli-0.16.8.tar.gz (67 kB)\n",
      "     |████████████████████████████████| 67 kB 4.3 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: sqlalchemy in /opt/conda/lib/python3.9/site-packages (from mlflow) (1.4.26)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.9/site-packages (from mlflow) (8.0.3)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.9/site-packages (from mlflow) (0.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from mlflow) (1.3.4)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.9/site-packages (from mlflow) (2.0.0)\n",
      "Requirement already satisfied: alembic in /opt/conda/lib/python3.9/site-packages (from mlflow) (1.7.4)\n",
      "Requirement already satisfied: requests>=2.17.3 in /opt/conda/lib/python3.9/site-packages (from mlflow) (2.26.0)\n",
      "Collecting docker>=4.0.0\n",
      "  Using cached docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
      "Collecting gunicorn\n",
      "  Using cached gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from mlflow) (1.7.2)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,>=3.7.0 in /opt/conda/lib/python3.9/site-packages (from mlflow) (4.8.1)\n",
      "Collecting sqlparse>=0.3.1\n",
      "  Using cached sqlparse-0.4.2-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from mlflow) (6.0)\n",
      "Collecting prometheus-flask-exporter\n",
      "  Using cached prometheus_flask_exporter-0.20.2-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.9/site-packages (from mlflow) (3.15.8)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.9/site-packages (from mlflow) (2021.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from mlflow) (1.21.4)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from databricks-cli>=0.8.7->mlflow) (2.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from databricks-cli>=0.8.7->mlflow) (3.1.1)\n",
      "Collecting tabulate>=0.7.7\n",
      "  Using cached tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.9/site-packages (from databricks-cli>=0.8.7->mlflow) (1.15.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.9/site-packages (from docker>=4.0.0->mlflow) (0.57.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata!=4.7.0,>=3.7.0->mlflow) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.17.3->mlflow) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.17.3->mlflow) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.17.3->mlflow) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.17.3->mlflow) (2.0.0)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.9/site-packages (from alembic->mlflow) (1.1.5)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.9/site-packages (from sqlalchemy->mlflow) (1.1.2)\n",
      "Requirement already satisfied: Werkzeug>=2.0 in /opt/conda/lib/python3.9/site-packages (from Flask->mlflow) (2.0.1)\n",
      "Collecting itsdangerous>=2.0\n",
      "  Using cached itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.9/site-packages (from Flask->mlflow) (3.0.2)\n",
      "Requirement already satisfied: setuptools>=3.0 in /opt/conda/lib/python3.9/site-packages (from gunicorn->mlflow) (58.5.3)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->mlflow) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->mlflow) (2.8.2)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.9/site-packages (from prometheus-flask-exporter->mlflow) (0.12.0)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from Jinja2>=3.0->Flask->mlflow) (2.0.1)\n",
      "Building wheels for collected packages: databricks-cli\n",
      "  Building wheel for databricks-cli (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for databricks-cli: filename=databricks_cli-0.16.8-py3-none-any.whl size=119421 sha256=b1531727d7cfc100dea4a9d4c48fdb0ecb90d09e043aac02bd9ebfb58772df93\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/6a/dd/3f/3b5f1b4edbbdf70adf45de8cae7491462012cb9d3cd67a23e5\n",
      "Successfully built databricks-cli\n",
      "Installing collected packages: smmap, itsdangerous, tabulate, gitdb, Flask, sqlparse, querystring-parser, prometheus-flask-exporter, gunicorn, gitpython, docker, databricks-cli, mlflow\n",
      "Successfully installed Flask-2.1.2 databricks-cli-0.16.8 docker-5.0.3 gitdb-4.0.9 gitpython-3.1.27 gunicorn-20.1.0 itsdangerous-2.1.2 mlflow-1.26.1 prometheus-flask-exporter-0.20.2 querystring-parser-1.2.4 smmap-5.0.0 sqlparse-0.4.2 tabulate-0.8.9\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/jovyan/Notebooks/mlruns/1', experiment_id='1', lifecycle_stage='active', name='predict_wine_quality', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "experiment_name = \"predict_wine_quality\"\n",
    "project_folder = 'wine'\n",
    "\n",
    "os.makedirs(project_folder, exist_ok = True)\n",
    "os.makedirs(project_folder + '/code', exist_ok = True)\n",
    "os.makedirs(project_folder + '/config', exist_ok = True)\n",
    "\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(experiment_name)\n",
    "except:\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    experiment_id = experiment.experiment_id\n",
    "    \n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Below is the script we wish to execute. A lot of the code should look familiar. Examine this script and try to point out the pieces that are new. What is the purpose of `sys.argv`? Notice how and where the `mlflow` library is used in the code. Finally, execute the script to make sure it works. There are several ways to execute a script:\n",
    "\n",
    "- from the **command line** navigate to its folder and run `python train.py`\n",
    "- from this **notebook** create a new cell and paste this `!python $project_folder/code/train.py`\n",
    "- from this **notebook** create a new cell and paste this `%run $project_folder/code/train.py`\n",
    "\n",
    "In order to execute the script make sure you first run the cell below. Note that if you changed the name of the experiment in cell above, you will need to also change it in the script in the cell below.\n",
    "\n",
    "### End of exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wine/code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $project_folder/code/train.py\n",
    "# The data set used in this example is from http://archive.ics.uci.edu/ml/datasets/Wine+Quality\n",
    "# P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.\n",
    "# Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from urllib.parse import urlparse\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level = logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    return rmse, mae, r2\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    np.random.seed(40)\n",
    "\n",
    "    # read the wine-quality csv file from the URL\n",
    "    csv_url = (\n",
    "        \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "    )\n",
    "    try:\n",
    "        data = pd.read_csv(csv_url, sep = \";\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\n",
    "            \"Unable to download training & test CSV, check your internet connection. Error: %s\", e\n",
    "        )\n",
    "\n",
    "    # split the data into training and test sets. (0.75, 0.25) split.\n",
    "    train, test = train_test_split(data)\n",
    "\n",
    "    # the predicted column is \"quality\" which is a scalar from [3, 9]\n",
    "    train_x = train.drop([\"quality\"], axis = 1)\n",
    "    test_x = test.drop([\"quality\"], axis = 1)\n",
    "    train_y = train[[\"quality\"]]\n",
    "    test_y = test[[\"quality\"]]\n",
    "\n",
    "    alpha = float(sys.argv[1]) if len(sys.argv) > 1 else 0.5\n",
    "    l1_ratio = float(sys.argv[2]) if len(sys.argv) > 2 else 0.5\n",
    "\n",
    "    mlflow.set_experiment(\"predict_wine_quality\")\n",
    "    # mlflow.autolog()\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        run = mlflow.active_run()\n",
    "        experiment = mlflow.get_experiment(run.info.experiment_id)\n",
    "        print(\"Experiment ID: \\\"{}\\\"\".format(run.info.experiment_id))\n",
    "        print(\"Experiment name: \\\"{}\\\"\".format(experiment.name))\n",
    "        print(\"Run ID: \\\"{}\\\"\".format(run.info.run_id))\n",
    "\n",
    "        lr = ElasticNet(alpha = alpha, l1_ratio = l1_ratio, random_state = 42)\n",
    "        lr.fit(train_x, train_y)\n",
    "\n",
    "        predicted_qualities = lr.predict(test_x)\n",
    "\n",
    "        (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "        print(\"Using alpha = {:0.2f}, l1_ratio = {:0.2f} we get the following metrics:\".format(alpha, l1_ratio))\n",
    "        print(\"  metric RMSE: {:6.2f}\".format(rmse))\n",
    "        print(\"  metric MAE: {:6.2f}\".format(mae))\n",
    "        print(\"  metric R-squared: {:0.2f}\".format(r2))\n",
    "\n",
    "        mlflow.log_param(\"alpha\", alpha)\n",
    "        mlflow.log_param(\"l1_ratio\", l1_ratio)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        # model registry does not work with file store\n",
    "        if tracking_url_type_store != \"file\":\n",
    "\n",
    "            # register the model\n",
    "            mlflow.sklearn.log_model(lr, \"model\", registered_model_name = \"ElasticnetWineModel\")\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(lr, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID: \"1\"\n",
      "Experiment name: \"predict_wine_quality\"\n",
      "Run ID: \"4b6827c35b0440c9a9e81457476cdfd7\"\n",
      "Using alpha = 0.50, l1_ratio = 0.50 we get the following metrics:\n",
      "  metric RMSE:   0.79\n",
      "  metric MAE:   0.63\n",
      "  metric R-squared: 0.11\n"
     ]
    }
   ],
   "source": [
    "!python $project_folder/code/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID: \"1\"\n",
      "Experiment name: \"predict_wine_quality\"\n",
      "Run ID: \"00973f39a40d41459fd7d5f82780943f\"\n",
      "Using alpha = 0.50, l1_ratio = 0.50 we get the following metrics:\n",
      "  metric RMSE:   0.79\n",
      "  metric MAE:   0.63\n",
      "  metric R-squared: 0.11\n"
     ]
    }
   ],
   "source": [
    "%run $project_folder/code/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we defined the above script with two inputs (what `mlflow` calls \"parameters\"), we can now change them to new values and execute the script again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID: \"1\"\n",
      "Experiment name: \"predict_wine_quality\"\n",
      "Run ID: \"988e111f74374ef49a07085443bc8b15\"\n",
      "Using alpha = 0.25, l1_ratio = 0.50 we get the following metrics:\n",
      "  metric RMSE:   0.75\n",
      "  metric MAE:   0.58\n",
      "  metric R-squared: 0.21\n"
     ]
    }
   ],
   "source": [
    "!python $project_folder/code/train.py 0.25 0.50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define an `mlflow` experiment and formalize what we did above. We create a file below that defines an `mlflow` project with its parameters and the command to be executed. Note that file paths are sepecified relative to the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wine/MLproject\n"
     ]
    }
   ],
   "source": [
    "%%writefile $project_folder/MLproject\n",
    "name: Wine Quality Prediction\n",
    "\n",
    "conda_env: config/conda.yaml\n",
    "\n",
    "entry_points:\n",
    "  main:\n",
    "    parameters:\n",
    "      alpha: float\n",
    "      l1_ratio: {type: float, default: 0.1}\n",
    "    command: \"python code/train.py {alpha} {l1_ratio}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above file also points to a conda environment file which we create below. This file defines the Python runtime used by the experiment. So for example, as part of the experiment, we can update one of the packages listed below and execute a new run to see if the update breaks our script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wine/config/conda.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $project_folder/config/conda.yaml\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - numpy=1.14.3\n",
    "  - pandas=0.22.0\n",
    "  - pip:\n",
    "    - mlflow\n",
    "    - scikit-learn==0.24.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute our experiment, we use the `mlflow` command. This is very similar to the way we executed the script earlier, but instead of pointing to the file we just provide the experiment name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022/06/15 17:18:48 INFO mlflow.utils.conda: === Creating conda environment mlflow-2f52bdca12d03fd90351043b6b674318958998c5 ===\n",
      "Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.3\n",
      "  latest version: 4.13.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "libstdcxx-ng-11.2.0  | 4.7 MB    | ##################################### | 100% \n",
      "xz-5.2.5             | 339 KB    | ##################################### | 100% \n",
      "zlib-1.2.12          | 106 KB    | ##################################### | 100% \n",
      "tbb-2021.5.0         | 157 KB    | ##################################### | 100% \n",
      "numpy-1.14.3         | 39 KB     | ##################################### | 100% \n",
      "mkl_fft-1.0.6        | 135 KB    | ##################################### | 100% \n",
      "readline-8.1.2       | 354 KB    | ##################################### | 100% \n",
      "ncurses-6.3          | 782 KB    | ##################################### | 100% \n",
      "sqlite-3.38.3        | 1.0 MB    | ##################################### | 100% \n",
      "python-3.6.13        | 32.5 MB   | ##################################### | 100% \n",
      "mkl_random-1.0.1     | 324 KB    | ##################################### | 100% \n",
      "pytz-2021.3          | 171 KB    | ##################################### | 100% \n",
      "ca-certificates-2022 | 124 KB    | ##################################### | 100% \n",
      "libgomp-11.2.0       | 474 KB    | ##################################### | 100% \n",
      "ld_impl_linux-64-2.3 | 654 KB    | ##################################### | 100% \n",
      "python-dateutil-2.8. | 233 KB    | ##################################### | 100% \n",
      "libffi-3.3           | 50 KB     | ##################################### | 100% \n",
      "mkl-2018.0.3         | 126.9 MB  | ##################################### | 100% \n",
      "pandas-0.22.0        | 8.2 MB    | ##################################### | 100% \n",
      "wheel-0.37.1         | 33 KB     | ##################################### | 100% \n",
      "tk-8.6.12            | 3.0 MB    | ##################################### | 100% \n",
      "blas-1.0             | 6 KB      | ##################################### | 100% \n",
      "pip-21.2.2           | 1.8 MB    | ##################################### | 100% \n",
      "_openmp_mutex-5.1    | 21 KB     | ##################################### | 100% \n",
      "intel-openmp-2022.0. | 4.2 MB    | ##################################### | 100% \n",
      "tbb4py-2021.3.0      | 82 KB     | ##################################### | 100% \n",
      "setuptools-58.0.4    | 788 KB    | ##################################### | 100% \n",
      "numpy-base-1.14.3    | 3.3 MB    | ##################################### | 100% \n",
      "libgfortran-ng-7.5.0 | 22 KB     | ##################################### | 100% \n",
      "six-1.16.0           | 18 KB     | ##################################### | 100% \n",
      "certifi-2021.5.30    | 139 KB    | ##################################### | 100% \n",
      "libgcc-ng-11.2.0     | 5.3 MB    | ##################################### | 100% \n",
      "openssl-1.1.1o       | 2.5 MB    | ##################################### | 100% \n",
      "libgfortran4-7.5.0   | 995 KB    | ##################################### | 100% \n",
      "_libgcc_mutex-0.1    | 3 KB      | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Installing pip dependencies: / Ran pip subprocess with arguments:\n",
      "['/opt/conda/envs/mlflow-2f52bdca12d03fd90351043b6b674318958998c5/bin/python', '-m', 'pip', 'install', '-U', '-r', '/home/jovyan/Notebooks/wine/config/condaenv.nldgpvty.requirements.txt']\n",
      "Pip subprocess output:\n",
      "Collecting mlflow\n",
      "  Using cached mlflow-1.23.1-py3-none-any.whl (15.6 MB)\n",
      "Collecting scikit-learn==0.24.1\n",
      "  Using cached scikit_learn-0.24.1-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/envs/mlflow-2f52bdca12d03fd90351043b6b674318958998c5/lib/python3.6/site-packages (from scikit-learn==0.24.1->-r /home/jovyan/Notebooks/wine/config/condaenv.nldgpvty.requirements.txt (line 2)) (1.14.3)\n",
      "Collecting scipy>=0.19.1\n",
      "  Using cached scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "Collecting click>=7.0\n",
      "  Using cached click-8.0.4-py3-none-any.whl (97 kB)\n",
      "Collecting sqlalchemy\n",
      "  Using cached SQLAlchemy-1.4.37-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "Collecting gitpython>=2.1.0\n",
      "  Using cached GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/mlflow-2f52bdca12d03fd90351043b6b674318958998c5/lib/python3.6/site-packages (from mlflow->-r /home/jovyan/Notebooks/wine/config/condaenv.nldgpvty.requirements.txt (line 1)) (0.22.0)\n",
      "Collecting requests>=2.17.3\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting entrypoints\n",
      "  Using cached entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Collecting protobuf>=3.7.0\n",
      "  Using cached protobuf-3.19.4-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting querystring-parser\n",
      "  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting Flask\n",
      "  Using cached Flask-2.0.3-py3-none-any.whl (95 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (603 kB)\n",
      "Collecting packaging\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting importlib-metadata!=4.7.0,>=3.7.0\n",
      "  Using cached importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
      "Collecting docker>=4.0.0\n",
      "  Using cached docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
      "Collecting prometheus-flask-exporter\n",
      "  Using cached prometheus_flask_exporter-0.20.2-py3-none-any.whl (18 kB)\n",
      "Collecting cloudpickle\n",
      "  Using cached cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
      "Collecting gunicorn\n",
      "  Using cached gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "Collecting databricks-cli>=0.8.7\n",
      "  Using cached databricks-cli-0.16.8.tar.gz (67 kB)\n",
      "Collecting alembic\n",
      "  Using cached alembic-1.7.7-py3-none-any.whl (210 kB)\n",
      "Collecting sqlparse>=0.3.1\n",
      "  Using cached sqlparse-0.4.2-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/mlflow-2f52bdca12d03fd90351043b6b674318958998c5/lib/python3.6/site-packages (from mlflow->-r /home/jovyan/Notebooks/wine/config/condaenv.nldgpvty.requirements.txt (line 1)) (2021.3)\n",
      "Collecting pyjwt>=1.7.0\n",
      "  Using cached PyJWT-2.4.0-py3-none-any.whl (18 kB)\n",
      "Collecting oauthlib>=3.1.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Collecting tabulate>=0.7.7\n",
      "  Using cached tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/envs/mlflow-2f52bdca12d03fd90351043b6b674318958998c5/lib/python3.6/site-packages (from databricks-cli>=0.8.7->mlflow->-r /home/jovyan/Notebooks/wine/config/condaenv.nldgpvty.requirements.txt (line 1)) (1.16.0)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Using cached websocket_client-1.3.1-py3-none-any.whl (54 kB)\n",
      "Collecting typing-extensions>=3.7.4.0\n",
      "  Using cached typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/mlflow-2f52bdca12d03fd90351043b6b674318958998c5/lib/python3.6/site-packages (from requests>=2.17.3->mlflow->-r /home/jovyan/Notebooks/wine/config/condaenv.nldgpvty.requirements.txt (line 1)) (2021.5.30)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting numpy>=1.13.3\n",
      "  Using cached numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "Collecting Mako\n",
      "  Using cached Mako-1.1.6-py2.py3-none-any.whl (75 kB)\n",
      "Collecting importlib-resources\n",
      "  Using cached importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Using cached greenlet-1.1.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n",
      "Collecting itsdangerous>=2.0\n",
      "  Using cached itsdangerous-2.0.1-py3-none-any.whl (18 kB)\n",
      "Collecting Jinja2>=3.0\n",
      "  Using cached Jinja2-3.0.3-py3-none-any.whl (133 kB)\n",
      "Collecting Werkzeug>=2.0\n",
      "  Using cached Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.0.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (30 kB)\n",
      "Collecting dataclasses\n",
      "  Using cached dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: setuptools>=3.0 in /opt/conda/envs/mlflow-2f52bdca12d03fd90351043b6b674318958998c5/lib/python3.6/site-packages (from gunicorn->mlflow->-r /home/jovyan/Notebooks/wine/config/condaenv.nldgpvty.requirements.txt (line 1)) (58.0.4)\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: python-dateutil>=2 in /opt/conda/envs/mlflow-2f52bdca12d03fd90351043b6b674318958998c5/lib/python3.6/site-packages (from pandas->mlflow->-r /home/jovyan/Notebooks/wine/config/condaenv.nldgpvty.requirements.txt (line 1)) (2.8.2)\n",
      "Collecting prometheus-client\n",
      "  Using cached prometheus_client-0.14.1-py3-none-any.whl (59 kB)\n",
      "Building wheels for collected packages: databricks-cli\n",
      "  Building wheel for databricks-cli (setup.py): started\n",
      "  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
      "  Created wheel for databricks-cli: filename=databricks_cli-0.16.8-py3-none-any.whl size=119422 sha256=bbe1b52856d159c3dcdb91321c8502d233c8f7881efbd8d4a776710ca364199b\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/ba/57/92/405538fabf279b256306ea331f45eeaa35c09b30a138519a81\n",
      "Successfully built databricks-cli\n",
      "Installing collected packages: zipp, typing-extensions, MarkupSafe, importlib-metadata, dataclasses, Werkzeug, urllib3, smmap, Jinja2, itsdangerous, idna, greenlet, click, charset-normalizer, websocket-client, tabulate, sqlalchemy, requests, pyparsing, pyjwt, prometheus-client, oauthlib, numpy, Mako, importlib-resources, gitdb, Flask, threadpoolctl, sqlparse, scipy, querystring-parser, pyyaml, protobuf, prometheus-flask-exporter, packaging, joblib, gunicorn, gitpython, entrypoints, docker, databricks-cli, cloudpickle, alembic, scikit-learn, mlflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.14.3\n",
      "    Uninstalling numpy-1.14.3:\n",
      "      Successfully uninstalled numpy-1.14.3\n",
      "Successfully installed Flask-2.0.3 Jinja2-3.0.3 Mako-1.1.6 MarkupSafe-2.0.1 Werkzeug-2.0.3 alembic-1.7.7 charset-normalizer-2.0.12 click-8.0.4 cloudpickle-2.1.0 databricks-cli-0.16.8 dataclasses-0.8 docker-5.0.3 entrypoints-0.4 gitdb-4.0.9 gitpython-3.1.18 greenlet-1.1.2 gunicorn-20.1.0 idna-3.3 importlib-metadata-4.8.3 importlib-resources-5.4.0 itsdangerous-2.0.1 joblib-1.1.0 mlflow-1.23.1 numpy-1.19.5 oauthlib-3.2.0 packaging-21.3 prometheus-client-0.14.1 prometheus-flask-exporter-0.20.2 protobuf-3.19.4 pyjwt-2.4.0 pyparsing-3.0.9 pyyaml-6.0 querystring-parser-1.2.4 requests-2.27.1 scikit-learn-0.24.1 scipy-1.5.4 smmap-5.0.0 sqlalchemy-1.4.37 sqlparse-0.4.2 tabulate-0.8.9 threadpoolctl-3.1.0 typing-extensions-4.1.1 urllib3-1.26.9 websocket-client-1.3.1 zipp-3.6.0\n",
      "\n",
      "done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate mlflow-2f52bdca12d03fd90351043b6b674318958998c5\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "2022/06/15 17:21:30 INFO mlflow.projects.utils: === Created directory /tmp/tmpwwc2gxo6 for downloading remote URIs passed to arguments of type 'path' ===\n",
      "2022/06/15 17:21:30 INFO mlflow.projects.backend.local: === Running command 'source activate mlflow-2f52bdca12d03fd90351043b6b674318958998c5 1>&2 && python code/train.py 0.42 0.1' in run with ID '71cdaf3aa6d148669824f581e4179b1b' === \n",
      "code/train.py:15: FutureWarning: MLflow support for Python 3.6 is deprecated and will be dropped in an upcoming release. At that point, existing Python 3.6 workflows that use MLflow will continue to work without modification, but Python 3.6 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3.7 or newer.\n",
      "  import mlflow\n",
      "Experiment ID: \"1\"\n",
      "Experiment name: \"predict_wine_quality\"\n",
      "Run ID: \"71cdaf3aa6d148669824f581e4179b1b\"\n",
      "Using alpha = 0.42, l1_ratio = 0.10 we get the following metrics:\n",
      "  metric RMSE:   0.74\n",
      "  metric MAE:   0.57\n",
      "  metric R-squared: 0.22\n",
      "2022/06/15 17:21:35 INFO mlflow.projects: === Run (ID '71cdaf3aa6d148669824f581e4179b1b') succeeded ===\n"
     ]
    }
   ],
   "source": [
    "!mlflow run $project_folder --experiment-name $experiment_name -P alpha=0.42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run the above command from the **command line**, as we will see in the following exercise. Finally, here's some useful information about our experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Let's look at some other examples of how `mlflow` works:\n",
    "\n",
    "- Running the above cell should show you some additional output besides the metrics. In the output, you should see the name of the conda environment created by `mlflow`. Launch the **Anaconda prompt** and activate the environment, then launch a Python session and check that the libraries match the versions specified in the conda environment we provided above. This can be useful for debugging your code. You can also learn about various `mlflow` commands by typing `mlflow --help`, `mlflow experiments --help` etc.\n",
    "- Return to activating the course environment (named `uwdatasci`), then from the **Anaconda prompt** navigate to the directory containing this notebook and then run `mlflow ui`. This will launch a new tab on your browser where you should now see an entry for the experiment we just ran. The user interface (UI) is where we go to look at all the runs in our experiment. Once in the `mlflow` UI, click on one of the successful runs and scroll down to **Artifacts** and look at the model artifact it saved for us. An example code is shown for how to load the artifact in a new Python session.\n",
    "- Return to the training script above, and uncomment the line `mlflow.autolog()`, then comment out the lines that use `log_param` and `log_metric` that tell us what parameters and metrics we want to log. Run the cell to save the new script, then submit a new run and go to the UI to compare the results with previous runs.\n",
    "\n",
    "![](../images/mlflow-ui-artifacts.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise\n",
    "\n",
    "Now let's see how we can load the model saved from one of our runs into the current Python session. To do so, we copy the line with `logged_model = ...` (see above screenshot) from the model artifacts page, and paste it below. We can then load a few rows of the wine data and use the model to get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: '/C:/Users/sethmott/OneDrive/Documents/UW/DATASCI-530/notebooks/mlruns/2/b752c0dc989e458db3940d195d4be348/artifacts/model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_53/1395190284.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogged_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'file:///C:/Users/sethmott/OneDrive/Documents/UW/DATASCI-530/notebooks/mlruns/2/b752c0dc989e458db3940d195d4be348/artifacts/model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogged_model\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# load model as a PyFuncModel.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_wine_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/wine.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'quality'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# load some data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_wine_sample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# predict on a pandas.DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_uri, suppress_warnings, dst_path)\u001b[0m\n\u001b[1;32m    714\u001b[0m                      \u001b[0mpath\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mcreated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m     \"\"\"\n\u001b[0;32m--> 716\u001b[0;31m     \u001b[0mlocal_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_download_artifact_from_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuppress_warnings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/mlflow/tracking/artifact_utils.py\u001b[0m in \u001b[0;36m_download_artifact_from_uri\u001b[0;34m(artifact_uri, output_path)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mroot_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlunparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     return get_artifact_repository(artifact_uri=root_uri).download_artifacts(\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0martifact_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0martifact_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     )\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/mlflow/store/artifact/local_artifact_repo.py\u001b[0m in \u001b[0;36mdownload_artifacts\u001b[0;34m(self, artifact_path, dst_path)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlocal_artifact_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifact_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_artifact_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No such file or directory: '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_artifact_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_artifact_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: No such file or directory: '/C:/Users/sethmott/OneDrive/Documents/UW/DATASCI-530/notebooks/mlruns/2/b752c0dc989e458db3940d195d4be348/artifacts/model'"
     ]
    }
   ],
   "source": [
    "logged_model = 'file:///C:/Users/sethmott/OneDrive/Documents/UW/DATASCI-530/notebooks/mlruns/2/b752c0dc989e458db3940d195d4be348/artifacts/model'\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model) # load model as a PyFuncModel.\n",
    "df_wine_sample = pd.read_csv('../data/wine.csv').drop(columns = ['quality', 'Class']).head() # load some data\n",
    "loaded_model.predict(df_wine_sample) # predict on a pandas.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the business need, we can also go one step further and serve the model over HTTP as a **scoring service**. This makes the model behave like an application. To do so, run the next cell, and copy its **output** and run it from the command line. Note that you can only run `mlflow` commands from the **Anaconda prompt** after activating the environment that `mlflow` is installed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo mlflow models serve -m $logged_model -p 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the output as you run the above command. We should see the conda environment being created before the model is served. Once the model is ready, the HTTP URL is shown as well.\n",
    "\n",
    "The data we send to the model must be in json format, which is one of the most command format that applications use to send data to each other. In this context, the data is sometimes referred to as the **payload**. Here is an example of what the data should look like in our case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/data/input_sample.json\n",
    "{\"columns\":[\"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\", \"chlorides\", \"free sulfur dioxide\", \"total sulfur dioxide\", \"density\", \"pH\", \"sulphates\", \"alcohol\"], \n",
    " \"index\":[0, 1, 2, 3, 4], \n",
    " \"data\":[\n",
    "     [7.4,  0.7,  0.0,  1.9, 0.076, 11.0, 34.0, 0.9978, 3.51, 0.56, 9.4], \n",
    "     [7.8,  0.88, 0.0,  2.6, 0.098, 25.0, 67.0, 0.9968, 3.2,  0.68, 9.8], \n",
    "     [7.8,  0.76, 0.04, 2.3, 0.092, 15.0, 54.0, 0.997,  3.26, 0.65, 9.8], \n",
    "     [11.2, 0.28, 0.56, 1.9, 0.075, 17.0, 60.0, 0.998,  3.16, 0.58, 9.8], \n",
    "     [7.4,  0.7,  0.0,  1.9, 0.076, 11.0, 34.0, 0.9978, 3.51, 0.56, 9.4]]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To send a request to the model, we can use the `curl` command, or any Rest API application like [Postman](https://www.postman.com/). Here is what the `curl` command looks like, which you can run on Linux or on Windows using [WSL](https://docs.microsoft.com/en-us/windows/wsl/install-win10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo curl -X POST -H \"Content-Type:application/json; format=pandas-split\" --data @$project_folder/data/input_sample.json http://127.0.0.1:1234/invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the output you should see by running the above command.\n",
    "\n",
    "    [5.422102809496764, 5.448114600770513, 5.444533999028288, 5.513957675441143, 5.422102809496764]\n",
    "    \n",
    "If we get errors two possible reasons are:\n",
    "- We need to first run `conda activate <environment-name>` to activate the Conda environment in which `mlflow` is installed.\n",
    "- We need to navigate to the folder where the notebook is running. This is because we set up the code so that paths are specified relative to this folder. You can run `print(os.getcwd())` to see the path, and then `cd` into it.\n",
    "\n",
    "Let's finish by pointing out two important aspects about `mlflow` here:\n",
    "- Everything we did here is \"local\", meaning that all meta-data is being saved to a local file path, but in most production system we use the cloud both for storage and for serving such models in production. For example, look [here](https://mlflow.org/docs/latest/models.html#deploy-a-python-function-model-on-microsoft-azure-ml) for an example of deployment in Azure. There are similar \"plug-ins\" for other cloud providers.\n",
    "- As we saw, there are three ways to interact with `mlflow`: through the Python library, through the command line, and through the UI. Which we use depends to some extent on what we want to do. For example, to log metrics, it makes sense to use the Python library and embed `mlflow` in the code. To run experiments and serve models we used the command line and to see and compare runs we used the UI, but in most cases we can also use the Python library, so it's a matter of preference to some extent. As an example, take a look at the next cell, which returns a `DataFrame` with meta-data for runs under our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.search_runs(experiment_id).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "In the lab, we saw how we can take a training script and use `mlflow` to submit runs. In practice, the training script isn't always clean code that is ready to use. So in this assignment, we learn to **refactor code** to train a model and prepare it for scoring. We learn about functionality in `sklearn` for **model persistence**, a fancy term for saving a model, so we can later load it and use it for prediction. We also learn how we can chain pre-processing steps and attach them to the model prediction step so we can both pre-process and score in one smooth flow.\n",
    "\n",
    "The bulk of the code that needs to execute is already given. This should look like code that we've written throughout the course. But when moving to production, it is **highly, highly recommended** that we **refactor** the code. What this means is that we need to go over the code from top to finish and do a bunch of things (now that we have the advantage of **hindsight**):\n",
    "\n",
    "- add **comments** in the code, for future us or (heaven forbid!) if someone else has to look at our code!\n",
    "- remove **extra code** that we wrote during development for debugging purposes but no longer need, or at least comment it out\n",
    "- simplify things, remove redundancies and make the code more **modular** by using functions or classes if needed\n",
    "- **parametrize** the code, so you avoid **hard-coding** things that need to change, and move them as high-level parameters at the top of the code, making it easy to change things without breaking things\n",
    "- create a runtime for the environment using Conda or PIP, which acts as a snapshot of the Python libraries we used and pins down their versions (careful: not all packages used during training are needed during scoring, and since the scoring enivironment is supposed to be **lightweight**, we should identify and remove such packages)\n",
    "- add **scaffolding**, this is to make sure that the code executes **gracefully** when errors happen, such as when the model expects a certain feature in the future data but it is missing for some reason (by gracefully, we mean that we use things like `try` and `except` to catch and redirect errors)\n",
    "- last but not least, never stop **testing**, but testing here can mean **unit testing**, **integration testing**, and even statistical tests for **data drift** or **model drift**\n",
    "- if you haven't yet (what have you been waiting for!) begin to **version control** your code using **git** or something similar\n",
    "\n",
    "Of course even with hindsight, doing these things is not easy and the answers are not always available, but we do the best we can and with experience we get better at it. Every project can be viewed as a work in progress, but applying certain **best practices** can make it easier to keep improving things without breaking them. This is what **agile development** is all about. You may have noticed that all the above steps are things we do generally when we write applications. It's just that not all data scientists have a rigorous computer science background and so we tend to have looser standards in general. The above is just the beginnig, not the end. Usually depending on the type of deployment we are doing, there are specific additional steps needed. \n",
    "\n",
    "Enough said. It's time for work! We used our knowledge of data science to write up some code to read data, pre-process it, train a model and use it to get predictions on a test data set. Here's a standard training code snippet. Examine it and run it to make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bank = pd.read_csv('../data/bank-full.csv', delimiter = ';')\n",
    "\n",
    "num_cols = bank.select_dtypes(['integer', 'float']).columns\n",
    "cat_cols = bank.select_dtypes(['object']).drop(columns = \"y\").columns\n",
    "\n",
    "print(\"Numeric columns are {}.\".format(\", \".join(num_cols)))\n",
    "print(\"Categorical columns are {}.\".format(\", \".join(cat_cols)))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bank.drop(columns = \"y\"), bank[\"y\"], \n",
    "                                                    test_size = 0.10, random_state = 42)\n",
    "\n",
    "X_train = X_train.reset_index(drop = True)\n",
    "X_test = X_test.reset_index(drop = True)\n",
    "\n",
    "print(\"Training data has {} rows.\".format(X_train.shape[0]))\n",
    "print(\"Test data has {} rows.\".format(X_test.shape[0]))\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "onehoter = OneHotEncoder(sparse = False)\n",
    "onehoter.fit(X_train[cat_cols])\n",
    "onehot_cols = onehoter.get_feature_names(cat_cols)\n",
    "X_train_onehot = pd.DataFrame(onehoter.transform(X_train[cat_cols]), columns = onehot_cols)\n",
    "X_test_onehot = pd.DataFrame(onehoter.transform(X_test[cat_cols]), columns = onehot_cols)\n",
    "\n",
    "znormalizer = StandardScaler()\n",
    "znormalizer.fit(X_train[num_cols])\n",
    "X_train_norm = pd.DataFrame(znormalizer.transform(X_train[num_cols]), columns = num_cols)\n",
    "X_test_norm = pd.DataFrame(znormalizer.transform(X_test[num_cols]), columns = num_cols)\n",
    "\n",
    "X_train_featurized = X_train_onehot # add one-hot-encoded columns\n",
    "X_test_featurized = X_test_onehot   # add one-hot-encoded columns\n",
    "X_train_featurized[num_cols] = X_train_norm # add numeric columns\n",
    "X_test_featurized[num_cols] = X_test_norm   # add numeric columns\n",
    "\n",
    "del X_train_norm, X_test_norm, X_train_onehot, X_test_onehot\n",
    "\n",
    "print(\"Featurized training data has {} rows and {} columns.\".format(*X_train_featurized.shape))\n",
    "print(\"Featurized test data has {} rows and {} columns.\".format(*X_test_featurized.shape))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit = LogisticRegression(max_iter = 5000, solver = 'lbfgs')\n",
    "logit.fit(X_train_featurized, y_train)\n",
    "\n",
    "y_hat_train = logit.predict(X_train_featurized)\n",
    "y_hat_test = logit.predict(X_test_featurized)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision_train = precision_score(y_train, y_hat_train, pos_label = 'yes') * 100\n",
    "precision_test = precision_score(y_test, y_hat_test, pos_label = 'yes') * 100\n",
    "\n",
    "recall_train = recall_score(y_train, y_hat_train, pos_label = 'yes') * 100\n",
    "recall_test = recall_score(y_test, y_hat_test, pos_label = 'yes') * 100\n",
    "\n",
    "print(\"Precision = {:.0f}% and recall = {:.0f}% on the training data.\".format(precision_train, recall_train))\n",
    "print(\"Precision = {:.0f}% and recall = {:.0f}% on the validation data.\".format(precision_test, recall_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above trains a model on data that contains both categorical and numeric features. We normalize the numeric features and one-hot-encode the categorical features as part of pre-processing. In the above code we do this \"manually\", however as shown [here](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer.html) we can compose data transformations and ML steps to create a **single multi-step pipeline**. The pipeline object (conviniently called `pipeline` in the docs) has a `fit` and `predict` method:\n",
    "- By calling `fit`, the raw input data is first transformed into the featurized data, and then passed to the ML algorithm to train a model.\n",
    "- By calling `predict`, the raw input data is first transformed into the featurized data (just like `fit`), and and then used to get predictions (using the model trained when we called `fit`).\n",
    "\n",
    "You can even create your own transformers and inculde them in a pipeline step, using as shown [here](https://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers), but we won't worry about this for this assignment.\n",
    "\n",
    "Let's also have some data that we can use for scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../data/new_data.json\n",
    "{'age': {'0': 40, '1': 47},\n",
    " 'balance': {'0': 580, '1': 3644},\n",
    " 'campaign': {'0': 1, '1': 2},\n",
    " 'contact': {'0': 'unknown', '1': 'unknown'},\n",
    " 'day': {'0': 16, '1': 9},\n",
    " 'default': {'0': 'no', '1': 'no'},\n",
    " 'duration': {'0': 192, '1': 83},\n",
    " 'education': {'0': 'secondary', '1': 'secondary'},\n",
    " 'housing': {'0': 'yes', '1': 'no'},\n",
    " 'job': {'0': 'blue-collar', '1': 'services'},\n",
    " 'loan': {'0': 'no', '1': 'no'},\n",
    " 'marital': {'0': 'married', '1': 'single'},\n",
    " 'month': {'0': 'may', '1': 'jun'},\n",
    " 'pdays': {'0': -1, '1': -1},\n",
    " 'poutcome': {'0': 'unknown', '1': 'unknown'},\n",
    " 'previous': {'0': 0, '1': 0}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run through the following steps to first refactor and test code:\n",
    "\n",
    "- **Step 1:** Compose the data processing and training steps in the above code into one pipeline as shown in the referenced doc. <span style=\"color:red\" float:right>[15 point]</span>\n",
    "- **Step 2:** Call `fit` and `predict` on the pipeline to make sure that it all works. Remember to pass them the **un-processed** (original) data, since the data processing should be built into the pipeline now. <span style=\"color:red\" float:right>[5 point]</span>\n",
    "- **Step 3:** Save your pipeline object using `joblib` as shown [here](https://sklearn.org/modules/model_persistence.html). <span style=\"color:red\" float:right>[5 point]</span>\n",
    "- **Step 4:** Now write a **new script** for scoring: it loads the pipeline you saved in the last step, reads the data `../data/new_data.json` and converts it to a `pandas.DataFrame` object, and obtains predictions on it. The predictions should be stored as a `json` file `../data/new_preds.json`. <span style=\"color:red\" float:right>[10 point]</span>\n",
    "\n",
    "To begin work on the assignemnt, it's best to first copy the training script in a cell below and begin modifying it according to the instructions in steps 1-3. Then create a new cell and populating with the scoring script described in step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modified training script goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scoring script goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of assignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
