{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we return to the topic of feature engineering from the previous lecture, but look at more complex examples of feature engineering. Many of these examples combine **domain knowledge** with machine learning. What this means is that the kind of data we have lends itself to certain types of features engineering that may not apply to other types of data. The best way to learn about this kind of feature engineering is by looking at examples: feature engineering with text data, with time series data, with image or video data, with sound data, and so on. So as our example, we will look at feature engineering for text data. To make things more interesting, we finish this notebook by training a supervised learning algorithm on the resulting data. This last part will warm us up for next week's lecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example dealing with text data. Raw text data is an unstructured and ubiquitous type of data. Most of the world’s data is unstructured. Volumes of unstructured data, including text, are growing much faster than structured data. There are many industry estimates for the fraction of all data which is unstructured. How much text data are we talking about here? In a few years time, Twitter will have [more text data recorded](http://www.internetlivestats.com/twitter-statistics/) than all that has been written in print in the history of mankind.\n",
    "\n",
    "Given the ubiquity and volume of text data, it is not surprising that numerous powerful applications which exploit text analytics are appearing. A few of these applications are listed below.\n",
    "\n",
    "- Intelligent applications\n",
    "  - Assistants\n",
    "  - Chat bots\n",
    "- Classification\n",
    "  - Sentiment analysis\n",
    "  - SPAM detection\n",
    "- Speech recognition\n",
    "- Search\n",
    "- Information retrieval\n",
    "- Legal discovery\n",
    "\n",
    "In this tutorial we investigate two areas of text analytics:\n",
    "\n",
    "- Pre-processing text data for analysis\n",
    "- Classification of text and sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cProfile\n",
    "import argparse\n",
    "import pprint\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# If you get an SSL-certificate error, and you are on a MAC then you may have to navigate to: application/python3/ and\n",
    "# run/double-click on the command 'install certificates'.  Then try this again.\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "###Sometimes if you have email addresses or numbers or letters that will not be relevant, should you keep them? remove them? etc...\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very first step to prepare text is to clean it.  We clean and normalize the text by performing various operations on the text. Some examples are as follows:\n",
    "\n",
    "- Make the text lowercase.\n",
    "- Remove symbols or punctuation.\n",
    "- Remove numbers. May also replace all numbers with a numeric tag, for example `<NUM>` or similar. We may also consider replacing all dates with `<DATE>` or similarly use tags `<URL>`, `<PHONE>`, `<EMAIL>`, etc...\n",
    "- Strip extra white space. White space has many forms: space, newline, or tab. There are also other rarely used unicode specifications for other white space characters.\n",
    "- Remove all non-printable unicode characters.\n",
    "- Replace accent characters.\n",
    "- Remove 'stop words'. Stop words are generally non-informative words like \"the\", \"as\", \"a\", etc.\n",
    "- Stem words to similar endings, such as \"eats\" and \"eat\".\n",
    "\n",
    "There are a few reasons to clean your text.  The primary reason is to reduce the potential vocabulary and increase the observations of specific words (or tokens). Depending on the application, the above steps should be considered carefully and only applied when it makes sense. Ask yourself if words like \"China\" and \"china\" to be different.\n",
    "\n",
    "**NOTE**: Be careful dealing with unicode characters. There are many editors and text viewers that only display printable characters but will not remove non-printable characters. Strange unicode characters can end up in data from users blindly copy/pasting text (with invisible unicode) into other text boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (8 minutes)\n",
    "\n",
    "By its very nature, text data comes unstructured and poorly organized for analysis. Typically multiple steps are required to process text into a form suitable for analysis, starting with cleaning it.\n",
    "\n",
    "Here's a horrible tweet. We're going to clean it and learn about ways to clean text data in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfiltered text: \n",
      "I <3 cleaning data $\\ \\ $, it’s my ၲ  $\\ \\ $    fAvoRitE!! 11!!! I enjoy looking for bats in the closet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "horrible_tweet_text = 'I <3 cleaning data $\\ \\ $, it’s my \\u1072  $\\ \\ $    fAvoRitE!! 11!!! I enjoy looking for bats in the closet'\n",
    "print('Unfiltered text: \\n{}\\n'.format(horrible_tweet_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step by step, refine the above text. To avoid overwriting the existing string, you can create a new string from the string so far each time.\n",
    "\n",
    "- Remove any non-ASCI character. A string `x` is ASCI if `ord(x) < 128`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =''.join([x for x in horrible_tweet_text if ord(x) < 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make all the letters lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i <3 cleaning data $\\\\ \\\\ $, its my   $\\\\ \\\\ $    favorite!! 11!!! i enjoy looking for bats in the closet'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all punctuation from the text. You can use `string.punctuation` to get a string of all punctuation characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed punctuation text: \n",
      "i  3 cleaning data         its my             favorite   11    i enjoy looking for bats in the closet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "punc = string.punctuation\n",
    "text = ''.join([' ' if ele in punc else ele for ele in text])\n",
    "\n",
    "print('Removed punctuation text: \\n{}\\n'.format(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the numbers from the text. HINT: If you are familiar with **regular expressions** you can use `re.sub` to do this easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed numbers text: \n",
      "i   cleaning data         its my             favorite       i enjoy looking for bats in the closet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = re.sub(r'\\d+', '', text)\n",
    "print('Removed numbers text: \\n{}\\n'.format(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Strip the text of any extra whitespace. HINT: The `split` method might help here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed whitespace spaces text: \n",
      "i cleaning data its my favorite i enjoy looking for bats in the closet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = ' '.join(text.split())\n",
    "print('Removed whitespace spaces text: \\n{}\\n'.format(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all stop words from the text. We can use `stopwords.words('english')` to get a list of stop words for the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated text: \n",
      " cleaning data   favorite  enjoy looking  bats   closet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = ' '.join(['' if ele in stopwords.words('english') else ele for ele in text.split()])\n",
    "\n",
    "print('updated text: \\n{}\\n'.format(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's now use `WordNetLemmatizer()` to reduce the words to their **stems**. HINT: Use the object's `lemmatize` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated text: \n",
      " cleaning data   favorite  enjoy looking  bat   closet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "word_list = text.split(' ')\n",
    "stemmed_words = [lmtzr.lemmatize(word) for word in word_list]\n",
    "text = ' '.join(stemmed_words) \n",
    "print('updated text: \\n{}\\n'.format(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now combine the steps outlined in the above exercise and create a function to clean text for use. We will later use this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: \"I <3 cleaning data $\\ \\ $, it’s my ၲ  $\\ \\ $    fAvoRitE!! 11!!! I enjoy looking for bats in the closet\"\n",
      "after : \"cleaning data favorite enjoy looking bat closet\"\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text, list_of_steps):\n",
    "    \n",
    "    for step in list_of_steps:\n",
    "        if step == 'remove_non_ascii':\n",
    "            text = ''.join([x for x in text if ord(x) < 128])\n",
    "        elif step == 'lowercase':\n",
    "            text = text.lower()\n",
    "        elif step == 'remove_punctuation':\n",
    "            punct_exclude = set(string.punctuation)\n",
    "            text = ''.join(char for char in text if char not in punct_exclude)\n",
    "        elif step == 'remove_numbers':\n",
    "            text = re.sub(\"\\d+\", \"\", text)\n",
    "        elif step == 'strip_whitespace':\n",
    "            text = ' '.join(text.split())\n",
    "        elif step == 'remove_stopwords':\n",
    "            stops = stopwords.words('english')\n",
    "            word_list = text.split(' ')\n",
    "            text_words = [word for word in word_list if word not in stops]\n",
    "            text = ' '.join(text_words)\n",
    "        elif step == 'stem_words':\n",
    "            lmtzr = WordNetLemmatizer()\n",
    "            word_list = text.split(' ')\n",
    "            stemmed_words = [lmtzr.lemmatize(word) for word in word_list]\n",
    "            text = ' '.join(stemmed_words)\n",
    "    return text\n",
    "\n",
    "step_list = ['remove_non_ascii', 'lowercase', 'remove_punctuation', 'remove_numbers',\n",
    "            'strip_whitespace', 'remove_stopwords', 'stem_words']\n",
    "\n",
    "print(\"before: \\\"{}\\\"\".format(horrible_tweet_text))\n",
    "print(\"after : \\\"{}\\\"\".format(preprocess(horrible_tweet_text, step_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurizing text data\n",
    "\n",
    "We the clean data we can begin to ask what is the best way to extract features from the data. There are many more approaches for text analytics and natural language processing (NLP). We only mention a few below. Note that the collection of unique words in the data is called a **corpus**. To avoid having a corpus that's too large, we can trim the corpus by keeping the most frequent $N$ words, making $N$ the size of the corpus. A **document** usually refers to a single data point with raw text, such as a tweet, a review, an invoice, etc. So our documents are made up of \"words\" that come from the corpus (ignoring any words that are not in the corpus). The question now is how do we represent such a data numerically? Here are two approaches:\n",
    "\n",
    "- The **bag of words model** is a simple and surprisingly effective model for analysis of text data. The BOW model creates a **sparse vector representation** of each word in the corpus based on the frequency of the words in the document. The order of the words is not considered, nor is the similarity between different words. Despite serious shortcomings, the model can work well in many cases.\n",
    "- We can usually do much better by using **word embeddings**, which are **dense vector respresentations** for each word in the corpus. Word embeddings are learned by examining the word's **context** (other words around it). Word embeddings are very common in **deep learning** applications of NLP, although the embeddings themselves are learned using a shallow network. If we learn word embeddings from a very large data set once, we can save and re-use these word embeddings to create features for other data sets. In fact, **pre-trained word embeddings** are trained by large companies like Google and made available for use by others. So we can load these embeddings and numerically represent a document using the average of the embeddings of the words in it. Because word embeddings are vectors, such an average would also be a vector that is a dense representation of the document.\n",
    "\n",
    "As you can see, BOW models seem too simplistic and word embeddings seem too sophisticated for now. So here's another approach that is sort of between the two in terms of difficulty. It is called TF-IDF and it is a clever way to featurize words in documents. Just like a BOW model, we begin by \"tokenizing\" the data. In BOW we then create a one-hot encoded feature for each token (or word). But in TF-IDF we first extract the relative word frequencies per document (called **term frequencies** or TF), we then multiply the term frequencies by a multiplier we call IDF. This has the effect of dampening the values for terms that appear frequently across documents, giving them less influence when we move on to the machine learning phase. Note that we used the words token, word and term almost interchangeably. Sorry for confusing you! Data scientists don't always agree on terminology. Let's dive in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text\n",
    "\n",
    "As a first step in preparing text for analysis of a document is to **tokenize** the text. In general terms, tokenization is the process dividing raw text into words, symbols and other elements, known as **tokens**. A set of tokens from all documents in the data is known as a **corpus**.\n",
    "\n",
    "As a first step in creating a corpus is reading the data set. This particular data set is comprised of 160,000 tweets. The sentiment of these tweets has been human labeled as positive or negative (4 is for positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>@elephantbird Hey dear, Happy Friday to You  A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Ughhh layin downnnn    Waiting for zeina to co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@greeniebach I reckon he'll play, even if he's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@vaLewee I know!  Saw it on the news!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>very sad that http://www.fabchannel.com/ has c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_label                                         tweet_text\n",
       "0                4  @elephantbird Hey dear, Happy Friday to You  A...\n",
       "1                4  Ughhh layin downnnn    Waiting for zeina to co...\n",
       "2                0  @greeniebach I reckon he'll play, even if he's...\n",
       "3                0              @vaLewee I know!  Saw it on the news!\n",
       "4                0  very sad that http://www.fabchannel.com/ has c..."
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = '../Data/twitter_data.csv'\n",
    "tweet_df = pd.read_csv(data_file)\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    26915\n",
       "1    26787\n",
       "Name: sentiment_label, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df['sentiment_label'] = tweet_df['sentiment_label'].replace(4, 1)\n",
    "tweet_df['sentiment_label'].value_counts()#0 is a negative message, 1 is a psoitive tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = tweet_df.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data set read, we need to clean then tokenize the tweets. Note that stemming can be slow on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = ['lowercase', 'remove_punctuation', 'remove_numbers', 'strip_whitespace', 'stem_words']\n",
    "tweet_df['clean_tweet'] = tweet_df['tweet_text'].map(lambda s: preprocess(s, steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@elephantbird Hey dear, Happy Friday to You  A...</td>\n",
       "      <td>elephantbird hey dear happy friday to you alre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ughhh layin downnnn    Waiting for zeina to co...</td>\n",
       "      <td>ughhh layin downnnn waiting for zeina to cook ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@greeniebach I reckon he'll play, even if he's...</td>\n",
       "      <td>greeniebach i reckon hell play even if he not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@vaLewee I know!  Saw it on the news!</td>\n",
       "      <td>valewee i know saw it on the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>very sad that http://www.fabchannel.com/ has c...</td>\n",
       "      <td>very sad that httpwwwfabchannelcom ha closed d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_label                                         tweet_text  \\\n",
       "0                1  @elephantbird Hey dear, Happy Friday to You  A...   \n",
       "1                1  Ughhh layin downnnn    Waiting for zeina to co...   \n",
       "2                0  @greeniebach I reckon he'll play, even if he's...   \n",
       "3                0              @vaLewee I know!  Saw it on the news!   \n",
       "4                0  very sad that http://www.fabchannel.com/ has c...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  elephantbird hey dear happy friday to you alre...  \n",
       "1  ughhh layin downnnn waiting for zeina to cook ...  \n",
       "2  greeniebach i reckon hell play even if he not ...  \n",
       "3                  valewee i know saw it on the news  \n",
       "4  very sad that httpwwwfabchannelcom ha closed d...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's sometimes helpful to profile such functions to find the main culprits and see if we can do anything to speed them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         15561035 function calls (15561032 primitive calls) in 6.641 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(copyto)\n",
      "        3    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1009(_handle_fromlist)\n",
      "    53702    0.332    0.000    6.524    0.000 <ipython-input-70-0f7b3a1f02b1>:1(preprocess)\n",
      "  3790685    0.452    0.000    0.452    0.000 <ipython-input-70-0f7b3a1f02b1>:10(<genexpr>)\n",
      "    53702    0.213    0.000    4.823    0.000 <ipython-input-70-0f7b3a1f02b1>:23(<listcomp>)\n",
      "    53702    0.080    0.000    6.604    0.000 <string>:1(<lambda>)\n",
      "        1    0.002    0.002    6.641    6.641 <string>:1(<module>)\n",
      "        2    0.000    0.000    0.000    0.000 _dtype.py:319(_name_includes_bit_suffix)\n",
      "        2    0.000    0.000    0.000    0.000 _dtype.py:333(_name_get)\n",
      "        2    0.000    0.000    0.000    0.000 _dtype.py:36(_kind_name)\n",
      "        2    0.000    0.000    0.000    0.000 _internal.py:830(npy_ctypes_check)\n",
      "        3    0.000    0.000    0.000    0.000 abc.py:137(__instancecheck__)\n",
      "        1    0.000    0.000    6.635    6.635 base.py:1078(_map_values)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:1374(nlevels)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:1642(is_unique)\n",
      "        5    0.000    0.000    0.000    0.000 base.py:256(is_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:4032(__contains__)\n",
      "        7    0.000    0.000    0.000    0.000 base.py:413(find)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:5553(ensure_index)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:5650(maybe_extract_name)\n",
      "        2    0.000    0.000    0.000    0.000 blocks.py:123(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 blocks.py:134(_check_ndim)\n",
      "        1    0.000    0.000    0.000    0.000 blocks.py:200(internal_values)\n",
      "        3    0.000    0.000    0.000    0.000 blocks.py:232(mgr_locs)\n",
      "        2    0.000    0.000    0.000    0.000 blocks.py:236(mgr_locs)\n",
      "        2    0.000    0.000    0.000    0.000 blocks.py:2371(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 blocks.py:255(make_block_same_class)\n",
      "        1    0.000    0.000    0.000    0.000 blocks.py:2652(get_block_type)\n",
      "        1    0.000    0.000    0.000    0.000 blocks.py:2698(make_block)\n",
      "        1    0.000    0.000    0.000    0.000 blocks.py:2725(_extend_blocks)\n",
      "        2    0.000    0.000    0.000    0.000 blocks.py:314(dtype)\n",
      "        1    0.000    0.000    0.001    0.001 blocks.py:513(astype)\n",
      "        1    0.000    0.000    0.001    0.001 blocks.py:674(copy)\n",
      "        1    0.000    0.000    0.000    0.000 cast.py:1180(maybe_castable)\n",
      "        1    0.000    0.000    0.000    0.000 cast.py:1194(maybe_infer_to_datetimelike)\n",
      "        1    0.000    0.000    0.000    0.000 cast.py:1303(maybe_cast_to_datetime)\n",
      "        1    0.000    0.000    0.000    0.000 cast.py:1588(construct_1d_ndarray_preserving_na)\n",
      "        6    0.000    0.000    0.000    0.000 common.py:1460(is_extension_array_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 common.py:1565(_get_dtype)\n",
      "        5    0.000    0.000    0.000    0.000 common.py:1600(_is_dtype_type)\n",
      "        3    0.000    0.000    0.000    0.000 common.py:1733(pandas_dtype)\n",
      "        4    0.000    0.000    0.000    0.000 common.py:178(classes)\n",
      "        4    0.000    0.000    0.000    0.000 common.py:180(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:183(classes_and_not_datetimelike)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:188(<lambda>)\n",
      "        4    0.000    0.000    0.000    0.000 common.py:194(is_object_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:224(is_sparse)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:329(apply_if_callable)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:381(is_datetime64tz_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:456(is_period_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:492(is_interval_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 common.py:530(is_categorical_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:608(is_dtype_equal)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:696(is_integer_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 construction.py:339(extract_array)\n",
      "        1    0.000    0.000    0.002    0.002 construction.py:390(sanitize_array)\n",
      "        1    0.000    0.000    0.000    0.000 construction.py:520(_try_cast)\n",
      "        1    0.000    0.000    0.000    0.000 construction.py:580(is_empty_data)\n",
      "        1    0.000    0.000    0.000    0.000 dtypes.py:1116(is_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 dtypes.py:903(is_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:2869(__getitem__)\n",
      "       26    0.000    0.000    0.000    0.000 generic.py:10(_check)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:195(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:232(attrs)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:3529(_get_item_cache)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:5092(__finalize__)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:5120(__getattr__)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:5138(__setattr__)\n",
      "        1    0.000    0.000    0.001    0.001 generic.py:5390(astype)\n",
      "        2    0.000    0.000    0.000    0.000 inference.py:263(is_dict_like)\n",
      "        4    0.000    0.000    0.000    0.000 inference.py:289(<genexpr>)\n",
      "        7    0.000    0.000    0.000    0.000 inference.py:322(is_hashable)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:72(isclass)\n",
      "        2    0.000    0.000    0.000    0.000 managers.py:1532(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:1553(from_blocks)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:1564(from_array)\n",
      "        2    0.000    0.000    0.000    0.000 managers.py:1575(_block)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:1602(dtype)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:1613(internal_values)\n",
      "        1    0.000    0.000    0.001    0.001 managers.py:366(apply)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:385(<dictcomp>)\n",
      "        1    0.000    0.000    0.001    0.001 managers.py:592(astype)\n",
      "        1    0.000    0.000    0.000    0.000 multiarray.py:1043(copyto)\n",
      "        1    0.000    0.000    0.000    0.000 numeric.py:150(is_all_dates)\n",
      "        1    0.000    0.000    0.000    0.000 numeric.py:283(full)\n",
      "        3    0.000    0.000    0.000    0.000 range.py:687(__len__)\n",
      "    53702    0.035    0.000    0.259    0.000 re.py:185(sub)\n",
      "    53702    0.051    0.000    0.071    0.000 re.py:271(_compile)\n",
      "        2    0.000    0.000    0.002    0.001 series.py:201(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 series.py:381(_constructor)\n",
      "        1    0.001    0.001    6.638    6.638 series.py:3895(map)\n",
      "        1    0.000    0.000    0.000    0.000 series.py:398(_set_axis)\n",
      "        1    0.000    0.000    0.000    0.000 series.py:427(dtype)\n",
      "        4    0.000    0.000    0.000    0.000 series.py:442(name)\n",
      "        4    0.000    0.000    0.000    0.000 series.py:492(name)\n",
      "        1    0.000    0.000    0.000    0.000 series.py:540(_values)\n",
      "   697029    0.796    0.000    3.962    0.000 wordnet.py:1873(_morphy)\n",
      "   704331    0.281    0.000    2.228    0.000 wordnet.py:1884(apply_rules)\n",
      "   704331    1.067    0.000    1.947    0.000 wordnet.py:1886(<listcomp>)\n",
      "   713504    0.842    0.000    0.938    0.000 wordnet.py:1892(filter_forms)\n",
      "    53702    0.008    0.000    0.008    0.000 wordnet.py:37(__init__)\n",
      "   697029    0.472    0.000    4.609    0.000 wordnet.py:40(lemmatize)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.callable}\n",
      "        1    0.000    0.000    6.641    6.641 {built-in method builtins.exec}\n",
      "       42    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method builtins.hash}\n",
      "    53780    0.020    0.000    0.020    0.000 {built-in method builtins.isinstance}\n",
      "       24    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "62108/62105    0.008    0.000    0.008    0.000 {built-in method builtins.len}\n",
      "   359786    0.175    0.000    0.175    0.000 {built-in method builtins.min}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method numpy.array}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.empty}\n",
      "   367452    0.043    0.000    0.043    0.000 {method 'add' of 'set' objects}\n",
      "   367453    0.053    0.000    0.053    0.000 {method 'append' of 'list' objects}\n",
      "        1    0.001    0.001    0.001    0.001 {method 'copy' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "  6345126    0.873    0.000    0.873    0.000 {method 'endswith' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "   161106    0.527    0.000    0.979    0.000 {method 'join' of 'str' objects}\n",
      "    53702    0.016    0.000    0.016    0.000 {method 'lower' of 'str' objects}\n",
      "   107404    0.107    0.000    0.107    0.000 {method 'split' of 'str' objects}\n",
      "    53702    0.154    0.000    0.154    0.000 {method 'sub' of 're.Pattern' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {pandas._libs.algos.ensure_object}\n",
      "        1    0.000    0.000    0.000    0.000 {pandas._libs.lib.infer_datetimelike_array}\n",
      "        1    0.002    0.002    0.002    0.002 {pandas._libs.lib.infer_dtype}\n",
      "        3    0.000    0.000    0.000    0.000 {pandas._libs.lib.is_list_like}\n",
      "        1    0.000    0.000    0.000    0.000 {pandas._libs.lib.item_from_zerodim}\n",
      "        1    0.030    0.030    6.634    6.634 {pandas._libs.lib.map_infer}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "cProfile.run(\"tweet_df['tweet_text'].map(lambda s: preprocess(s, steps))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** To help us understand TF-IDF in great detail, we learn about TF-IDF in two different ways:\n",
    "\n",
    "- We first run through the steps \"manually\", so that we can see talk about implementation details. \n",
    "- We then apply TF-IDF using `sklearn` in a few lines of code, which will hide all of that detail.\n",
    "\n",
    "## Computing TF-IDF\n",
    "\n",
    "Computing the TF-IDF values manually consist of several steps:\n",
    "1. We obtain the term-document matirx. \n",
    "1. We calculate the **term-frequency (TF)** matirx, where $\\text{TF(doc, term)}$ is the term frequencies from the term document matrix divided by the total number of terms in the document. In other words, we turn the frequencies into percentages (also called relative frequencies).\n",
    "1. We trim the term-document matrix if needed. This gets rid of useless words that frequently appear across documents.\n",
    "1. We can also derive the **inverse document frequencies (IDF)** matrix:\n",
    "\n",
    "   $$\\text{IDF(term)} = \\log(\\frac{\\text{number of documents}}{\\text{number of documents with term in it} + 1})$$\n",
    "\n",
    "   Note that TF is a function of both term and document, but the IDF is just a function of the terms.\n",
    "1. We multiply the TF with IDF values to get TF-IDF values. This results in **term frequency inverse document frequency (TF-IDF) maxtrix**. \n",
    "   \n",
    "   $$\\text{TF-IDF} = \\text{TF(term, doc)} \\cdot \\text{IDF(term)}$$\n",
    "\n",
    "To get an intuition behind what's happening, here's what the IDF function looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0, 100, num = 80)\n",
    "sns.lineplot(x = x, y = np.log(100 / (x + 1)));###Equation above, have plus 1 so you never have 0 in the denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFs give us the relative frequency terms in the documents. We use the IDFs to then dampen the TFs for terms that appear frequently across documents by virtue of being common terms. This will make it so rarer terms get to exert more influence during the machine learning phase.\n",
    "\n",
    "Applications of TF-IDF include\n",
    "\n",
    "- Characterize writing styles\n",
    "- Comparing authors\n",
    "- Determining original authors\n",
    "- Finding plagiarism\n",
    "\n",
    "Let's now implement this for the tweet data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get the term-document matrix\n",
    "\n",
    "We first break up the words in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry: ['chillin', 'in', 'the', 'hotel', 'room', 'were', 'about', 'to', 'check', 'out', 'the', 'city', 'richmond', 'va', 'then', 'head', 'home', 'gonna', 'b', 'traveling', 'aaaaallll', 'day']\n"
     ]
    }
   ],
   "source": [
    "clean_texts = tweet_df['clean_tweet']\n",
    "docs = {}\n",
    "labels = []\n",
    "for ix, row in enumerate(clean_texts):\n",
    "    labels = tweet_data[ix][0]\n",
    "    docs[ix] = row.split(' ')\n",
    "\n",
    "print('Example entry: {}'.format(docs[np.random.choice(ix)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our corpus, keeping track of its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our tweet-vocabulary has 64566 distinct words.\n"
     ]
    }
   ],
   "source": [
    "num_nonzero = 0\n",
    "vocab = set()\n",
    "\n",
    "for word_list in docs.values():\n",
    "    unique_terms = set(word_list)    # all unique terms of this tweet\n",
    "    vocab.update(unique_terms)       # set union: add unique terms of this tweet\n",
    "    num_nonzero += len(unique_terms) # add count of unique terms in this tweet\n",
    "\n",
    "doc_key_list = list(docs.keys())\n",
    "\n",
    "print('Our tweet-vocabulary has {} distinct words.'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now convert everything to a numpy array. We should keep track of how the vocab/term indices map to the matrix so that we can look them up later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['anthmauro' 'postmemorial' 'allowance' 'blessed' 'uplifter']\n",
      "Sorted Vocab: ['a' 'aa' 'aaa' 'aaaaa' 'aaaaaa' 'aaaaaaaaaaaaaaaa'\n",
      " 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa' 'aaaaaaaaaaaaaaah'\n",
      " 'aaaaaaaaaaaaaaahmmmmmmmmmm' 'aaaaaaaaaaaaaah'\n",
      " 'aaaaaaaaaaaaaahhhhhhhhhhhhhhhhhhhhhhhhhhhhhh' 'aaaaaaaaand' 'aaaaaaaall'\n",
      " 'aaaaaaaaw' 'aaaaaaah' 'aaaaaagh' 'aaaaaah' 'aaaaaaw' 'aaaaache' 'aaaaah']\n"
     ]
    }
   ],
   "source": [
    "doc_key_list = np.array(doc_key_list)\n",
    "vocab = np.array(list(vocab))\n",
    "\n",
    "vocab_sorter = np.argsort(vocab)\n",
    "\n",
    "print('Vocab: {}'.format(vocab[:5]))\n",
    "print('Sorted Vocab: {}'.format(vocab[vocab_sorter[:20]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now initialize the **term-document matrix**. This is a matrix where each row $i$ is a document and each column $j$ is a word. The entriy $(i, j)$ of the matrix shows the frequency of the word $j$ in document $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = len(doc_key_list)\n",
    "vocab_size = len(vocab)\n",
    "data = np.empty(num_nonzero, dtype = np.intc)     # all non-zero\n",
    "rows = np.empty(num_nonzero, dtype = np.intc)     # row index\n",
    "cols = np.empty(num_nonzero, dtype = np.intc)     # column index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now populate the term-document matrix: Each row $i$ is a document and each column $j$ is a word. The entriy $(i, j)$ of the matrix shows the frequency of the word $j$ in document $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing, please wait!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "ix = 0\n",
    "# go through all documents with their terms\n",
    "print('Computing, please wait!')\n",
    "for doc_key, terms in docs.items():\n",
    "    # find indices to insert-into such that, if the corresponding elements were\n",
    "    # inserted before the indices, the order would be preserved\n",
    "    term_indices = vocab_sorter[np.searchsorted(vocab, terms, sorter = vocab_sorter)]\n",
    "\n",
    "    # count the unique terms of the document and get their vocabulary indices\n",
    "    uniq_indices, counts = np.unique(term_indices, return_counts = True)\n",
    "    n_vals = len(uniq_indices)  # number of unique terms\n",
    "    ix_end = ix + n_vals # add count to index\n",
    "\n",
    "    data[ix:ix_end] = counts                  # save the counts (term frequencies)\n",
    "    cols[ix:ix_end] = uniq_indices            # save the column index: index in \n",
    "    doc_ix = np.where(doc_key_list == doc_key)   # get the document index for the document name\n",
    "    rows[ix:ix_end] = np.repeat(doc_ix, n_vals)  # save it as repeated value\n",
    "\n",
    "    ix = ix_end  # resume with next document -> will add future data on the end\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our sorted vocabulary again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five terms alphabetically: ['a' 'aa' 'aaa' 'aaaaa' 'aaaaaa']\n"
     ]
    }
   ],
   "source": [
    "print('First five terms alphabetically: {}'.format(vocab[vocab_sorter[:5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we probably need to do some trimming, as the word 'aaaaa' probably doesn't occur often enough, and having 151,670 unique words may be too much.  We will address this later on. For now, let's keep the corpus we have. We now initialize a sparse matrix that will store the term-document matirx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<53702x64566 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 658645 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_mat = coo_matrix((data, (rows, cols)), shape = (num_docs, vocab_size), dtype = np.intc)\n",
    "\n",
    "doc_term_mat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now populate the matrix with the term frequencies. We can see an example of this for the word \"python\" in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab index of python : 35861\n",
      "\n",
      "1st document index containing said word: 14592\n",
      "\n",
      "Tweet: [1, \"omg, I just found my old vid of monty python's life of brian. it's brilliant, I just can't get enough stan today\"]\n"
     ]
    }
   ],
   "source": [
    "# let's check to make sure!\n",
    "vocab_list = list(vocab)\n",
    "word_of_interest = 'python'\n",
    "vocab_interesting_ix = list(vocab).index(word_of_interest)\n",
    "print('vocab index of {} : {}'.format(word_of_interest, vocab_interesting_ix))\n",
    "# find which tweets contain word\n",
    "doc_ix_with_word = []\n",
    "for ix, row in enumerate(tweet_data): # note on this line later\n",
    "    if word_of_interest in row[1]:\n",
    "        doc_ix_with_word.append(ix)\n",
    "\n",
    "print('\\n1st document index containing said word: {}'.format(doc_ix_with_word[0]))\n",
    "print('\\nTweet: {}'.format(tweet_data[doc_ix_with_word[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the word \"python\" appears once in the first document (tweet).\n",
    "\n",
    "**Note:** The term-document matirx `doc_term_mat` is a sparse matirx. So we can't index it using row and column index. Here we don't really need to do that, but if we did we would first need to convert it into an array. We can use the `toarray` method to do that, but if the matrix is large we can easily run out of memory (that's why we're using a spares matrix in the first place!). So instead we can use the `tocrs` method shown below, which uses compression to avoid the memory problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Row 14592 and column 35861 of document-term matrix has entry 1\n"
     ]
    }
   ],
   "source": [
    "# document - term matrix relevant entry\n",
    "document_row = doc_ix_with_word[0]\n",
    "vocab_col = vocab_interesting_ix\n",
    "mat_entry = doc_term_mat.tocsr()[document_row, vocab_col]\n",
    "\n",
    "print('\\nRow {} and column {} of document-term matrix has entry {}'.format(document_row, vocab_col, mat_entry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Trimming the document-term matrix\n",
    "\n",
    "We saw above that we are including terms like 'aaaaa' and 'aaaa', which probably occur very few times. These terms generally occur with unstructured text fields because we allow users to input whatever they feel like and that includes typos.  But be aware that they can also be artifacts of our cleaning process (unintentionally and intentionally).\n",
    "\n",
    "Since our document-term matrix is a matrix of counts of words (columns) in each document (rows), we want to remove words that don't occur very frequently across our corpus. The count of how frequent a word is in all of our corpus is just the sum of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 1 3 1]]\n"
     ]
    }
   ],
   "source": [
    "word_counts = doc_term_mat.sum(axis = 0)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how many words are above a specific cutoff, such as 15:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words w/counts above 15 : 2805\n"
     ]
    }
   ],
   "source": [
    "cutoff = 15\n",
    "word_count_list = word_counts.tolist()[0]\n",
    "col_cutoff_ix = [ix for ix, count in enumerate(word_count_list) if count > cutoff]\n",
    "\n",
    "print('Number of words w/counts above {} : {}'.format(cutoff, len(col_cutoff_ix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now trim our vocabulary and document term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of document-term matrix before trimming: (53702, 64566)\n",
      "Shape of document-term matrix after trimming: (53702, 2805)\n"
     ]
    }
   ],
   "source": [
    "vocab_trimmed = np.array([vocab[x] for x in col_cutoff_ix])\n",
    "vocab_sorter_trimmed = np.argsort(vocab_trimmed)\n",
    "\n",
    "print('Shape of document-term matrix before trimming: {}'.format(doc_term_mat.shape))\n",
    "\n",
    "doc_term_mat_trimmed = doc_term_mat.tocsc()[:, col_cutoff_ix]\n",
    "print('Shape of document-term matrix after trimming: {}'.format(doc_term_mat_trimmed.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'aaron', 'able', 'about', 'absolutely', 'abt', 'ac', 'accent',\n",
       "       'accept', 'access'], dtype='<U19')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at first 10 words alphabetically\n",
    "vocab_trimmed[vocab_sorter_trimmed[0:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know what cutoff we should use? Let's look at a bar plot words in descending frequency before and after we trimmed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = pd.value_counts(word_count_list, normalize = True)[:20]\n",
    "words = [vocab[x] for x in counts.index][:20]\n",
    "g = sns.barplot(x = words, y = counts, color = 'lightblue')\n",
    "g.set_xticklabels(rotation = 90, labels = words);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the bar plot is very skewed. There are too many words that appear few times. Let's check out the trimmed list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trimmed_word_counts = doc_term_mat_trimmed.sum(axis = 0)\n",
    "trimmed_word_list = trimmed_word_counts.tolist()[0]\n",
    "\n",
    "counts = pd.value_counts(trimmed_word_list, normalize = True)[:20]\n",
    "counts = counts.reset_index(drop = True)\n",
    "words = [vocab[x] for x in counts.index][:20]\n",
    "g = sns.barplot(x = words, y = counts, color = 'lightblue')\n",
    "g.set_xticklabels(rotation = 90, labels = words);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that 15 might be a good cutoff because we get a visible drop off and around 6000 features, but these types of **hyperparameters** for the model will probably need tuning. We will learn about tuning such values in future lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Compute IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Term Frequency\n",
    "\n",
    "Now that we have computed a document-term matrix, how can we understand it? Recall that the simple **Bag of Words model** is just based on **Term Frequency (TF)**. In this case, the weighting of a document for a given term is just the frequency of that term in the document. \n",
    "\n",
    "In other cases we will used the **Inverse Document Frequency (IDF)** weighting. IDF weighting accounts for cases where only a few documents contain certain terms. The formula for the IDF weighting can be written as:\n",
    "\n",
    "$$IDF = log(\\frac{Number\\ Documents}{Number\\ Documents\\ with\\ Word})$$\n",
    "\n",
    "The IDF can exhibit a problem however. When there are a few documents with very frequent terms, the weighting is skewed toward those documents.  To solve this problem, we reweight IDF by the overall frequency of the word to create a **term frequency-inverse document frequency (TF-IDF)** matrix. The formula for computing TFIDF is: \n",
    "\n",
    "$$TF - IDF = frequency(word) \\cdot log(\\frac{Number\\ Documents}{Number\\ Documents\\ with\\ Word})\\ $$\n",
    "\n",
    "The code in the cell below computes both simple TF and the cumulative of the term frequencies, starting from the most frequent terms to the least.\n",
    "\n",
    "Scikit-Learn has a built in TF-IDF transformation function that we will use to calculate this and prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts = tweet_df['clean_tweet']\n",
    "# Declare the TFIDF vectorizer.\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, max_features=5856, stop_words='english')#Can change dictionary from 'english' to 'french' if you'd like\n",
    "\n",
    "# Fit the vectorizer over the dataset\n",
    "tf_idf_tweets = vectorizer.fit_transform(clean_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<53702x5856 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 294937 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scikit learn prefers the 'csr' format instead (Compressed Sparse Row format)\n",
    "tf_idf_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Classification and sentiment analysis\n",
    "\n",
    "It's hard to talk about feature engineering without moving on to the next step: training a machine learning model. Although we cover this topic on a future lesson, we include here an example in case you can't wait any longer!\n",
    "\n",
    "Now that we have a prepared TDM of the 160,000 tweets, let's build and evaluate models to classify the sentiment of these tweets. The idea is simple: We use the TF-IDF features for training the model. Since our data also has a column that says if the tweet expresses a positive or negative sentiment, we will train a model to predict the sentiment from the TF-IDF features. So first let's obtain the TF-IDF features once more:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the featurized data into training and test sets. We will explain why in a future lecture. For training we will use 120,000 tweets to predict the 0,1 sentiment. The remaining 40,000 cases will be used to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_targets = np.array([y[0] for y in tweet_data])\n",
    "y_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_tweets, y_targets, test_size = 40000, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a logistic classifier on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_probs = lr.predict_proba(X_train)\n",
    "train_results = np.argmax(train_probs, axis=1)\n",
    "\n",
    "test_probs = lr.predict_proba(X_test)\n",
    "test_results = np.argmax(test_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8252809808787038\n",
      "Test accuracy: 0.7372\n"
     ]
    }
   ],
   "source": [
    "train_logical_correct = [pred == actual for pred, actual in zip(train_results, y_train)]\n",
    "train_acc = np.mean(train_logical_correct)\n",
    "\n",
    "test_logical_correct = [pred == actual for pred, actual in zip(test_results, y_test)]\n",
    "test_acc = np.mean(test_logical_correct)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute the precision, recall and Fscore of the model for positive and negative tweets.\n",
    "\n",
    "Recall that a positive prediction here means a positive review, and so **precision** is the proportion of correct predictions among all positive predictions and **recall** is the proportion of correct predictions among all true positives. **F1** is the harmonic average of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14515  5487]\n",
      " [ 5025 14973]]\n",
      "===================================\n",
      "             Class 1   -   Class 0\n",
      "Precision: [0.74283521 0.73181818]\n",
      "Recall   : [0.72567743 0.74872487]\n",
      "F1       : [0.73415609 0.740175  ]\n",
      "Support  : [20002 19998]\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, test_results)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, test_results).ravel()\n",
    "\n",
    "print(confusion_matrix(y_test, test_results))\n",
    "print('='*35)\n",
    "print('             Class 1   -   Class 0')\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall   : {}'.format(recall))\n",
    "print('F1       : {}'.format(f1))\n",
    "print('Support  : {}'.format(support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Summary\n",
    "\n",
    "NLP applications extend far and wide, so we only stratched the surface here. Many of the modern breakthroughs in deep learning for example have been in NLP. One reason for this is that language data is abundant and the lack of structure in the data presents us with many challenges and learning opportunities. We hope this notebook exposed you to just some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment (Milestone 2)\n",
    "\n",
    "We saw how TF-IDF can be used to create features on text data. Let's now look at an example of a special transformation very common in the retail industry: RFM or recency-frequency-monetary transformation. The goal of this assignment is to implement create RFM features for the `retail-churn.csv` data. You will see that having time series data opens us up to many types of features (although how useful they will ultimately be is another question).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the `retail-churn.csv` data we are by now very familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>address</th>\n",
       "      <th>store_id</th>\n",
       "      <th>trans_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "      <th>quantity</th>\n",
       "      <th>dollar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101981</td>\n",
       "      <td>F</td>\n",
       "      <td>E</td>\n",
       "      <td>2860</td>\n",
       "      <td>818463</td>\n",
       "      <td>11/1/2000 0:00</td>\n",
       "      <td>4.710000e+12</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101981</td>\n",
       "      <td>F</td>\n",
       "      <td>E</td>\n",
       "      <td>2861</td>\n",
       "      <td>818464</td>\n",
       "      <td>11/1/2000 0:00</td>\n",
       "      <td>4.710000e+12</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101981</td>\n",
       "      <td>F</td>\n",
       "      <td>E</td>\n",
       "      <td>2862</td>\n",
       "      <td>818465</td>\n",
       "      <td>11/1/2000 0:00</td>\n",
       "      <td>4.710000e+12</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101981</td>\n",
       "      <td>F</td>\n",
       "      <td>E</td>\n",
       "      <td>2863</td>\n",
       "      <td>818466</td>\n",
       "      <td>11/1/2000 0:00</td>\n",
       "      <td>4.710000e+12</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101981</td>\n",
       "      <td>F</td>\n",
       "      <td>E</td>\n",
       "      <td>2864</td>\n",
       "      <td>818467</td>\n",
       "      <td>11/1/2000 0:00</td>\n",
       "      <td>4.710000e+12</td>\n",
       "      <td>8</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id gender address  store_id  trans_id       timestamp       item_id  \\\n",
       "0   101981      F       E      2860    818463  11/1/2000 0:00  4.710000e+12   \n",
       "1   101981      F       E      2861    818464  11/1/2000 0:00  4.710000e+12   \n",
       "2   101981      F       E      2862    818465  11/1/2000 0:00  4.710000e+12   \n",
       "3   101981      F       E      2863    818466  11/1/2000 0:00  4.710000e+12   \n",
       "4   101981      F       E      2864    818467  11/1/2000 0:00  4.710000e+12   \n",
       "\n",
       "   quantity  dollar  \n",
       "0         1      37  \n",
       "1         1      17  \n",
       "2         1      23  \n",
       "3         1      41  \n",
       "4         8     288  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = ['user_id', 'gender', 'address', 'store_id', 'trans_id', 'timestamp', 'item_id', 'quantity', 'dollar']\n",
    "churn = pd.read_csv(\"../Data/retail-churn.csv\", sep = \",\", skiprows = 1, names = col_names)\n",
    "churn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following steps to feature engineer the data.\n",
    "\n",
    "1. Convert the `timestamp` column to be of type `datetime`. <span style=\"color:red\" float:right>[1 point]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasoning - To be able to switch a column to a datetime type, I will need to use the pd.to_datetime call and pass the necessary column through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id               int64\n",
       "gender               object\n",
       "address              object\n",
       "store_id              int64\n",
       "trans_id              int64\n",
       "timestamp    datetime64[ns]\n",
       "item_id             float64\n",
       "quantity              int64\n",
       "dollar                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn['timestamp'] = pd.to_datetime(churn['timestamp'])\n",
    "churn.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion - after passing the pd.to_datetime call, I was able to call the churn.dtypes to see that it is now in datetime format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Extract the date from `datetime` and store it in a new column called `date`. <span style=\"color:red\" float:right>[1 point]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasoning - Will use the dt.strftime call to get the data into a format I can work with for later and also use the pd.to_datetime to store it into a new column called 'date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2000-11-01\n",
       "1   2000-11-01\n",
       "2   2000-11-01\n",
       "3   2000-11-01\n",
       "4   2000-11-01\n",
       "Name: date, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn['date']=churn['timestamp'].dt.strftime(\"%Y-%m-%d\")\n",
    "churn['date'] = pd.to_datetime(churn['date'])\n",
    "churn['date'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-11-01 00:00:00\n",
      "2000-12-12 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#Taking a look at the temporal difference\n",
    "print(churn['date'].min())\n",
    "print(churn['date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id               int64\n",
       "gender               object\n",
       "address              object\n",
       "store_id              int64\n",
       "trans_id              int64\n",
       "timestamp    datetime64[ns]\n",
       "item_id             float64\n",
       "quantity              int64\n",
       "dollar                int64\n",
       "date         datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making sure that the new column is in datetime format\n",
    "churn.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion - By using the strftime command and then the pd.to_datetime, I was able to create 'date' and then convert it into a datetime column. Also found out that my distribution is from January to December of 2000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the **granularity** of the data is not daily spend, but rather individual transactions. We can see that because the same user has multiple transactions with the same timestamp. Before we run RFM, we need to **aggregate** the data so we have daily granularity.\n",
    "\n",
    "3. Aggregate `quantity` and `dollar` to daily data (so that `user_id` and `date` are unique for each row). Call the aggregated data `churn_agg`. You can ignore all the other columns, as they are not needed. <span style=\"color:red\" float:right>[2 point]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasoning - To get the user_id and date, I will need to use a groupby, and then aggregate based on quantity (sum) and dollar (sum) to obtain the correct df, churn_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id              int64\n",
       "date        datetime64[ns]\n",
       "quantity             int64\n",
       "dollar               int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#as_index=False - in groupby function\n",
    "churn_agg = churn.groupby(['user_id', 'date'], as_index=False).agg({'quantity':'sum', 'dollar':'sum'})\n",
    "churn_agg.head()\n",
    "churn_agg.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    13328.000000\n",
      "mean         9.811900\n",
      "std         18.276495\n",
      "min          1.000000\n",
      "25%          3.000000\n",
      "50%          7.000000\n",
      "75%         12.000000\n",
      "max       1200.000000\n",
      "Name: quantity, dtype: float64\n",
      "count    13328.000000\n",
      "mean       891.074205\n",
      "std       1338.841301\n",
      "min          9.000000\n",
      "25%        228.000000\n",
      "50%        512.000000\n",
      "75%       1100.000000\n",
      "max      62688.000000\n",
      "Name: dollar, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(churn_agg.quantity.describe())\n",
    "print(churn_agg.dollar.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'date', 'quantity', 'dollar'], dtype='object')\n",
      "user_id              int64\n",
      "date        datetime64[ns]\n",
      "quantity             int64\n",
      "dollar               int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(churn_agg.columns)\n",
    "print(churn_agg.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/how-to-group-data-by-different-time-intervals-using-python-pandas-eb7134f9b9b0\n",
    "\n",
    "https://pbpython.com/pandas-grouper-agg.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion - By using groupby on user_id and date, I was able to groupby them and then aggregate based on the quantity and dollar that each consumer (user_id) purchased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Using the aggregated data, obtain recency, frequency and monetary features for both `dollar` and `quantity`. Use a 7-day moving window for frequency and monetary. Call your new features `last_visit_ndays` (recency) `quantity_roll_sum_7D` (frequency) and `dollar_roll_sum_7D` (monetary). <span style=\"color:red\" float:right>[4 point]</span>\n",
    "\n",
    "  HINT: In `pandas` recency is a kind of **difference** feature, because it's based on calculating the difference between the current date and a previous date (called a **lag**). We can use the `diff` method to get recency. Frequency and monetary features are called **rolling** features, because it is a type of cumulative sum but over a moving window. We can use the `rolling` function to get frequency and monetary, where the `window` and `on` arguments need to chosen carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasoning - I will do this question in parts:\n",
    "1) To get the last visit in days I will use the diff() call after grouping the user_id based on date\n",
    "\n",
    "2) I will then use the rolling() function on both the frequency and monetary over 7 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/how-to-group-data-by-different-time-intervals-using-python-pandas-eb7134f9b9b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>quantity</th>\n",
       "      <th>dollar</th>\n",
       "      <th>last_visit_ndays</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1113</td>\n",
       "      <td>2000-11-12</td>\n",
       "      <td>5</td>\n",
       "      <td>420</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1113</td>\n",
       "      <td>2000-11-26</td>\n",
       "      <td>3</td>\n",
       "      <td>558</td>\n",
       "      <td>14 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1113</td>\n",
       "      <td>2000-11-27</td>\n",
       "      <td>6</td>\n",
       "      <td>624</td>\n",
       "      <td>1 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1823</td>\n",
       "      <td>2000-11-02</td>\n",
       "      <td>16</td>\n",
       "      <td>1256</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1823</td>\n",
       "      <td>2000-11-06</td>\n",
       "      <td>6</td>\n",
       "      <td>918</td>\n",
       "      <td>4 days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id       date  quantity  dollar last_visit_ndays\n",
       "0     1113 2000-11-12         5     420              NaT\n",
       "1     1113 2000-11-26         3     558          14 days\n",
       "2     1113 2000-11-27         6     624           1 days\n",
       "3     1823 2000-11-02        16    1256              NaT\n",
       "4     1823 2000-11-06         6     918           4 days"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_agg['last_visit_ndays']=churn_agg.groupby('user_id')['date'].diff()\n",
    "churn_agg.head()\n",
    "#last_visit_ndays = churn_agg.groubpy('user_id')['date'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_agg.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    13328.000000\n",
      "mean        13.941252\n",
      "std         31.006416\n",
      "min          1.000000\n",
      "25%          5.000000\n",
      "50%          9.000000\n",
      "75%         17.000000\n",
      "max       1659.000000\n",
      "Name: quantity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "quantity_roll_sum_7D = churn_agg.groupby('user_id').rolling('7D')['quantity'].sum()\n",
    "print(quantity_roll_sum_7D.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    13328.000000\n",
      "mean      1257.541792\n",
      "std       2052.527607\n",
      "min          9.000000\n",
      "25%        343.000000\n",
      "50%        747.000000\n",
      "75%       1502.250000\n",
      "max      66974.000000\n",
      "Name: dollar, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "dollar_roll_sum_7D = churn_agg.groupby('user_id').rolling('7D')['dollar'].sum()\n",
    "print(dollar_roll_sum_7D.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion - Was able to get all three parts finished and sorted, only thing was when I tried to set the last_visit_ndays as its own series it would not let me, is that an issue with the index???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Combine all three features into a single `DataFrame` and call it `churn_roll`. <span style=\"color:red\" float:right>[1 point]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasoning - This one I will need to combine the quantity_roll, dollar_roll, and the last visitndays. Based on above, I will need to seperate out the last_visit_n_days and then combine it with the quantity roll and dollar roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>quantity</th>\n",
       "      <th>dollar</th>\n",
       "      <th>last_visit_ndays</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-11-12</th>\n",
       "      <td>1113</td>\n",
       "      <td>5</td>\n",
       "      <td>420</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-26</th>\n",
       "      <td>1113</td>\n",
       "      <td>3</td>\n",
       "      <td>558</td>\n",
       "      <td>14 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-27</th>\n",
       "      <td>1113</td>\n",
       "      <td>6</td>\n",
       "      <td>624</td>\n",
       "      <td>1 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-02</th>\n",
       "      <td>1823</td>\n",
       "      <td>16</td>\n",
       "      <td>1256</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-06</th>\n",
       "      <td>1823</td>\n",
       "      <td>6</td>\n",
       "      <td>918</td>\n",
       "      <td>4 days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id  quantity  dollar last_visit_ndays\n",
       "date                                                  \n",
       "2000-11-12     1113         5     420              NaT\n",
       "2000-11-26     1113         3     558          14 days\n",
       "2000-11-27     1113         6     624           1 days\n",
       "2000-11-02     1823        16    1256              NaT\n",
       "2000-11-06     1823         6     918           4 days"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id  date      \n",
      "1113     2000-11-12     5.0\n",
      "         2000-11-26     3.0\n",
      "         2000-11-27     9.0\n",
      "1823     2000-11-02    16.0\n",
      "         2000-11-06    22.0\n",
      "Name: quantity, dtype: float64\n",
      "user_id  date      \n",
      "1113     2000-11-12     420.0\n",
      "         2000-11-26     558.0\n",
      "         2000-11-27    1182.0\n",
      "1823     2000-11-02    1256.0\n",
      "         2000-11-06    2174.0\n",
      "Name: dollar, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(quantity_roll_sum_7D.head())\n",
    "print(dollar_roll_sum_7D.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>quantity</th>\n",
       "      <th>dollar</th>\n",
       "      <th>last_visit_ndays</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-11-12</td>\n",
       "      <td>1113</td>\n",
       "      <td>5</td>\n",
       "      <td>420</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-11-26</td>\n",
       "      <td>1113</td>\n",
       "      <td>3</td>\n",
       "      <td>558</td>\n",
       "      <td>14 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-11-27</td>\n",
       "      <td>1113</td>\n",
       "      <td>6</td>\n",
       "      <td>624</td>\n",
       "      <td>1 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-11-02</td>\n",
       "      <td>1823</td>\n",
       "      <td>16</td>\n",
       "      <td>1256</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-11-06</td>\n",
       "      <td>1823</td>\n",
       "      <td>6</td>\n",
       "      <td>918</td>\n",
       "      <td>4 days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  user_id  quantity  dollar last_visit_ndays\n",
       "0 2000-11-12     1113         5     420              NaT\n",
       "1 2000-11-26     1113         3     558          14 days\n",
       "2 2000-11-27     1113         6     624           1 days\n",
       "3 2000-11-02     1823        16    1256              NaT\n",
       "4 2000-11-06     1823         6     918           4 days"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_agg.reset_index(inplace=True)\n",
    "churn_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>quantity</th>\n",
       "      <th>dollar</th>\n",
       "      <th>last_visit_ndays</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1113</th>\n",
       "      <th>2000-11-12</th>\n",
       "      <td>5</td>\n",
       "      <td>420</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-26</th>\n",
       "      <td>3</td>\n",
       "      <td>558</td>\n",
       "      <td>14 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-27</th>\n",
       "      <td>6</td>\n",
       "      <td>624</td>\n",
       "      <td>1 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1823</th>\n",
       "      <th>2000-11-02</th>\n",
       "      <td>16</td>\n",
       "      <td>1256</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-06</th>\n",
       "      <td>6</td>\n",
       "      <td>918</td>\n",
       "      <td>4 days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    quantity  dollar last_visit_ndays\n",
       "user_id date                                         \n",
       "1113    2000-11-12         5     420              NaT\n",
       "        2000-11-26         3     558          14 days\n",
       "        2000-11-27         6     624           1 days\n",
       "1823    2000-11-02        16    1256              NaT\n",
       "        2000-11-06         6     918           4 days"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_agg.set_index(['user_id', 'date'], inplace=True)\n",
    "churn_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id  date      \n",
       "1113     2000-11-12       NaT\n",
       "         2000-11-26   14 days\n",
       "         2000-11-27    1 days\n",
       "1823     2000-11-02       NaT\n",
       "         2000-11-06    4 days\n",
       "Name: last_visit_ndays, dtype: timedelta64[ns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_visit_n_days_roll = churn_agg['last_visit_ndays']\n",
    "last_visit_n_days_roll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    quantity_roll_sum_7D  dollar_roll_sum_7D last_visit_ndays\n",
      "user_id date                                                                 \n",
      "1113    2000-11-12                   5.0               420.0              NaT\n",
      "        2000-11-26                   3.0               558.0          14 days\n",
      "        2000-11-27                   9.0              1182.0           1 days\n",
      "1823    2000-11-02                  16.0              1256.0              NaT\n",
      "        2000-11-06                  22.0              2174.0           4 days\n"
     ]
    }
   ],
   "source": [
    "churn_roll = pd.concat([quantity_roll_sum_7D, dollar_roll_sum_7D, last_visit_n_days_roll], axis=1)\n",
    "\n",
    "churn_roll.rename(columns = {'quantity':'quantity_roll_sum_7D', 'dollar':'dollar_roll_sum_7D', 'recency':'last_visit_n_days_roll'}, inplace=True)\n",
    "print(churn_roll.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion - I was able to merge the three series by manipulating the indexes and working out how to join them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/community/tutorials/introduction-customer-segmentation-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Use `fillna` to replace missing values for recency with a large value like 100 days (whatever makes business sense). HINT: You can use `pd.Timedelta('100 days')` to set the value. <span style=\"color:red\" float:right>[1 point]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasoning - I will cal lthe fill.na(pd.Timedelta('100 days') to populate those values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id  date      \n",
       "1113     2000-11-12   100 days\n",
       "         2000-11-26    14 days\n",
       "         2000-11-27     1 days\n",
       "1823     2000-11-02   100 days\n",
       "         2000-11-06     4 days\n",
       "                        ...   \n",
       "2163925  2000-12-10   100 days\n",
       "2163956  2000-12-10   100 days\n",
       "2164007  2000-12-10   100 days\n",
       "2164083  2000-12-11   100 days\n",
       "2164175  2000-12-11   100 days\n",
       "Name: last_visit_ndays, Length: 13328, dtype: timedelta64[ns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_roll['last_visit_ndays'].fillna(pd.Timedelta('100 days'), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion - The call worked by going over all of the Na values and populating them with 100 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. To see if things worked, merge the aggregated data `churn_agg` with the RFM features in `churn_roll`. You can use the `merge` method to do this with the right keys specified. <span style=\"color:red\" float:right>[2 point]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasoning - Will use the drop call since I believe that I have the last_visit_ndays in both dataframes at this point, after doing that I will merge the two dataframes using the merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    quantity_roll_sum_7D  dollar_roll_sum_7D last_visit_ndays\n",
      "user_id date                                                                 \n",
      "1113    2000-11-12                   5.0               420.0              NaT\n",
      "        2000-11-26                   3.0               558.0          14 days\n",
      "        2000-11-27                   9.0              1182.0           1 days\n",
      "1823    2000-11-02                  16.0              1256.0              NaT\n",
      "        2000-11-06                  22.0              2174.0           4 days\n",
      "                    quantity  dollar last_visit_ndays\n",
      "user_id date                                         \n",
      "1113    2000-11-12         5     420              NaT\n",
      "        2000-11-26         3     558          14 days\n",
      "        2000-11-27         6     624           1 days\n",
      "1823    2000-11-02        16    1256              NaT\n",
      "        2000-11-06         6     918           4 days\n"
     ]
    }
   ],
   "source": [
    "print(churn_roll.head())\n",
    "print(churn_agg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13328, 3)\n",
      "(13328, 2)\n"
     ]
    }
   ],
   "source": [
    "print(churn_agg.shape)\n",
    "churn_agg.drop('last_visit_ndays',axis=1, inplace=True)\n",
    "print(churn_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    quantity  dollar\n",
      "user_id date                        \n",
      "1113    2000-11-12         5     420\n",
      "        2000-11-26         3     558\n",
      "        2000-11-27         6     624\n",
      "1823    2000-11-02        16    1256\n",
      "        2000-11-06         6     918\n"
     ]
    }
   ],
   "source": [
    "print(churn_agg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(churn_agg,churn_roll, right_index = True, \n",
    "               left_index = True)\n",
    "merged['last_visit_ndays'].fillna(pd.Timedelta('100 days'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>quantity</th>\n",
       "      <th>dollar</th>\n",
       "      <th>quantity_roll_sum_7D</th>\n",
       "      <th>dollar_roll_sum_7D</th>\n",
       "      <th>last_visit_ndays</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1113</th>\n",
       "      <th>2000-11-12</th>\n",
       "      <td>5</td>\n",
       "      <td>420</td>\n",
       "      <td>5.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>100 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-26</th>\n",
       "      <td>3</td>\n",
       "      <td>558</td>\n",
       "      <td>3.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>14 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-27</th>\n",
       "      <td>6</td>\n",
       "      <td>624</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1182.0</td>\n",
       "      <td>1 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1823</th>\n",
       "      <th>2000-11-02</th>\n",
       "      <td>16</td>\n",
       "      <td>1256</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1256.0</td>\n",
       "      <td>100 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-06</th>\n",
       "      <td>6</td>\n",
       "      <td>918</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>4 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4947</th>\n",
       "      <th>2000-12-03</th>\n",
       "      <td>24</td>\n",
       "      <td>1875</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>100 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-12-10</th>\n",
       "      <td>16</td>\n",
       "      <td>1488</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1488.0</td>\n",
       "      <td>7 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">6668</th>\n",
       "      <th>2000-11-11</th>\n",
       "      <td>6</td>\n",
       "      <td>432</td>\n",
       "      <td>6.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>100 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-25</th>\n",
       "      <td>11</td>\n",
       "      <td>873</td>\n",
       "      <td>11.0</td>\n",
       "      <td>873.0</td>\n",
       "      <td>14 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-12-03</th>\n",
       "      <td>7</td>\n",
       "      <td>737</td>\n",
       "      <td>7.0</td>\n",
       "      <td>737.0</td>\n",
       "      <td>8 days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    quantity  dollar  quantity_roll_sum_7D  \\\n",
       "user_id date                                                 \n",
       "1113    2000-11-12         5     420                   5.0   \n",
       "        2000-11-26         3     558                   3.0   \n",
       "        2000-11-27         6     624                   9.0   \n",
       "1823    2000-11-02        16    1256                  16.0   \n",
       "        2000-11-06         6     918                  22.0   \n",
       "4947    2000-12-03        24    1875                  24.0   \n",
       "        2000-12-10        16    1488                  16.0   \n",
       "6668    2000-11-11         6     432                   6.0   \n",
       "        2000-11-25        11     873                  11.0   \n",
       "        2000-12-03         7     737                   7.0   \n",
       "\n",
       "                    dollar_roll_sum_7D last_visit_ndays  \n",
       "user_id date                                             \n",
       "1113    2000-11-12               420.0         100 days  \n",
       "        2000-11-26               558.0          14 days  \n",
       "        2000-11-27              1182.0           1 days  \n",
       "1823    2000-11-02              1256.0         100 days  \n",
       "        2000-11-06              2174.0           4 days  \n",
       "4947    2000-12-03              1875.0         100 days  \n",
       "        2000-12-10              1488.0           7 days  \n",
       "6668    2000-11-11               432.0         100 days  \n",
       "        2000-11-25               873.0          14 days  \n",
       "        2000-12-03               737.0           8 days  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion - By removing the extra column it decreased the repetitivty of my columns in the dataframe and then allowed for a smooth merge of the two dataframes (churn_roll and churn_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Check the features we created to make sure they appear to show the right calculations. You can do this by just checking the first 10 rows of the data. <span style=\"color:red\" float:right>[1 point]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasoning - To call the first 10 rows of the merged data I will use the .head(10), will also take a look at the churn_roll and churn_agg .head to make sure they are all on the same page, so to stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    quantity  dollar  quantity_roll_sum_7D  \\\n",
      "user_id date                                                 \n",
      "1113    2000-11-12         5     420                   5.0   \n",
      "        2000-11-26         3     558                   3.0   \n",
      "        2000-11-27         6     624                   9.0   \n",
      "1823    2000-11-02        16    1256                  16.0   \n",
      "        2000-11-06         6     918                  22.0   \n",
      "4947    2000-12-03        24    1875                  24.0   \n",
      "        2000-12-10        16    1488                  16.0   \n",
      "6668    2000-11-11         6     432                   6.0   \n",
      "        2000-11-25        11     873                  11.0   \n",
      "        2000-12-03         7     737                   7.0   \n",
      "\n",
      "                    dollar_roll_sum_7D last_visit_ndays  \n",
      "user_id date                                             \n",
      "1113    2000-11-12               420.0              NaT  \n",
      "        2000-11-26               558.0          14 days  \n",
      "        2000-11-27              1182.0           1 days  \n",
      "1823    2000-11-02              1256.0              NaT  \n",
      "        2000-11-06              2174.0           4 days  \n",
      "4947    2000-12-03              1875.0              NaT  \n",
      "        2000-12-10              1488.0           7 days  \n",
      "6668    2000-11-11               432.0              NaT  \n",
      "        2000-11-25               873.0          14 days  \n",
      "        2000-12-03               737.0           8 days  \n"
     ]
    }
   ],
   "source": [
    "print(merged.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    quantity  dollar\n",
      "user_id date                        \n",
      "1113    2000-11-12         5     420\n",
      "        2000-11-26         3     558\n",
      "        2000-11-27         6     624\n",
      "1823    2000-11-02        16    1256\n",
      "        2000-11-06         6     918\n",
      "4947    2000-12-03        24    1875\n",
      "        2000-12-10        16    1488\n",
      "6668    2000-11-11         6     432\n",
      "        2000-11-25        11     873\n",
      "        2000-12-03         7     737\n"
     ]
    }
   ],
   "source": [
    "print(churn_agg.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    quantity_roll_sum_7D  dollar_roll_sum_7D last_visit_ndays\n",
      "user_id date                                                                 \n",
      "1113    2000-11-12                   5.0               420.0              NaT\n",
      "        2000-11-26                   3.0               558.0          14 days\n",
      "        2000-11-27                   9.0              1182.0           1 days\n",
      "1823    2000-11-02                  16.0              1256.0              NaT\n",
      "        2000-11-06                  22.0              2174.0           4 days\n",
      "4947    2000-12-03                  24.0              1875.0              NaT\n",
      "        2000-12-10                  16.0              1488.0           7 days\n",
      "6668    2000-11-11                   6.0               432.0              NaT\n",
      "        2000-11-25                  11.0               873.0          14 days\n",
      "        2000-12-03                   7.0               737.0           8 days\n"
     ]
    }
   ],
   "source": [
    "print(churn_roll.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion - The merge seemed to work well and the data is all set and snug in the right frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One take-away from the above example is that feature engineering can be a complicated topic, and relies to some extent on creativity and domain knowledge, as we saw with time series data and RFM. For this reason, some modern machine learning libraries are working on what is called **automated feature engineering** to see if algorithms can automatically figure out a set of good features to use by the machine learning model.\n",
    "\n",
    "# End of assignment"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
